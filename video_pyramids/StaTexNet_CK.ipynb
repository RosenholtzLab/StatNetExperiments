{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c9fe52-26af-4089-803e-4406a29432f3",
   "metadata": {},
   "source": [
    "# StaTexNet - I honestly don't know whether this does correct things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc538b7-27ab-46c2-be44-33847dda73a7",
   "metadata": {},
   "source": [
    "## Dependencies & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e95b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:44:59.904549200Z",
     "start_time": "2023-07-27T14:44:59.889457100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.9/site-packages/')\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PooledStatisticsMetamers/poolstatmetamer/')\n",
    "import utils.statnetencoder as sne\n",
    "import importlib\n",
    "import imp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\chris\\Documents\\MIT\\Statistics_analysis_code\\PyTorchSteerablePyramid')\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PyTorchSteerablePyramid/')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb413b8-72ca-4a7a-b98f-d32cc73b5470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:47:05.643243100Z",
     "start_time": "2023-07-27T14:47:05.619221Z"
    }
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "num_epochs = 50\n",
    "batch_size = 20\n",
    "crop_size = 128 \n",
    "num_stats = 50 \n",
    "optimizer_type='adam'\n",
    "#optimizer_type='sgd'\n",
    "learning_rate = 0.001\n",
    "num_crops = 5 #changed this to four\n",
    "\n",
    "multistat_penalty = 0\n",
    "sparsity_penalty = 0.01\n",
    "entropic_penalty = 0\n",
    "\n",
    "#dataset location\n",
    "#dtd_folder = '/gridsan/ckoevesdi/data/dtd_torch/dt/'\n",
    "dtd_folder = 'home/gridsan/ckoevesdi/data/OT/dtd_torch/dtd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806091f-63d7-4819-ad4e-2ef117a7097d",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1482b4a6-f29a-4448-be62-51cf8ae7defd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T15:37:37.244268300Z",
     "start_time": "2023-07-27T15:36:26.383047900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = torchvision.transforms.Compose([#transforms.CenterCrop(size=300),\n",
    "                                                    #transforms.RandomRotation(degrees=180),\n",
    "                                                    transforms.Grayscale(), \n",
    "                                                    #transforms.TenCrop(size=crop_size),\n",
    "                                                    #transforms.RandomRotation(degrees=[0,90,180,270]),\n",
    "                                                    transforms.RandomVerticalFlip(p=0.5), #Commented out\n",
    "                                                    transforms.RandomHorizontalFlip(p=0.5), #Commented out\n",
    "                                                    transforms.FiveCrop(size=crop_size), #changed back to fivecrop\n",
    "                                                    #transforms.functional.vflip(),\n",
    "                                                    #transforms.functional.hflip(),\n",
    "                                                    transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),\n",
    "                                                    transforms.ConvertImageDtype(torch.float32)])\n",
    "                                                    #transforms.PILToTensor()])\n",
    "\n",
    "#use training set for now\n",
    "dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='train', partition=1, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "\n",
    "sampler = data.RandomSampler(dtd_dataset)\n",
    "\n",
    "dtd_dataloader = DataLoader(dtd_dataset, \n",
    "                            sampler=sampler,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d800e7a-2310-48a7-a262-d9360b1e7f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solver: pyramid=UBBBBL_6 pooling=WholeImagePooling()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import utils.brucenet as bn\n",
    "brucy = bn.BruceNet(pooling_region_size=1e20, pyramid_params=False, dummy_img = torch.zeros(20,\n",
    "                                     1,\n",
    "                                     128,\n",
    "                                     128)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59334fe-e567-4931-b257-e7e1e339f0c3",
   "metadata": {},
   "source": [
    "## Checking which statistics really are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c0e7d-307f-418b-8e22-39fa7bee80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the image\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/')\n",
    "\n",
    "#### IMAGE INPUT\n",
    "image = cv2.imread('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/cracked_0004.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "x = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "x = x/255.0\n",
    "x_expanded = np.expand_dims(x, axis=0)\n",
    "y = np.stack([x_expanded,x_expanded])\n",
    "y_t = torch.from_numpy(y).to(dtype=torch.float32, device=device)\n",
    "print(y_t.shape)\n",
    "\n",
    "#### IMAGE OUTPUT\n",
    "image2 = cv2.imread('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/cracked_0004_output.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "x2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "x2 = x2/255.0\n",
    "x2_expanded = np.expand_dims(x2, axis=0)\n",
    "y2 = np.stack([x2_expanded,x2_expanded])\n",
    "y2_t = torch.from_numpy(y2).to(dtype=torch.float32, device=device)\n",
    "\n",
    "print(y2_t.shape)\n",
    "\n",
    "z = brucy(y_t)\n",
    "print(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437b6a4-9a74-4958-bc5d-878781cfa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = brucy(y_t)\n",
    "z2 = brucy(y2_t)\n",
    "print(z2)\n",
    "\n",
    "#ordered_stats_labels_pretty = [' '.join([''.join(str(item)) for item in row if item not in ['',None]]) for row in stat_labels]\n",
    "#print(ordered_stats_labels_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da452e-c47c-4501-ace2-7fac9fff5607",
   "metadata": {},
   "source": [
    "## Checking the distance between crops from same and from different image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48501d09-5210-48b9-9fe0-ecb2d305d4a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 1, 128, 128])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([20, 5, 1, 128, 128])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([20, 5, 1, 128, 128])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([5, 150])\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    for j, (texture_batch, labels) in enumerate(dtd_dataloader):\n",
    "        print(texture_batch.shape)\n",
    "        output = texture_batch[0].to(device)\n",
    "        x = brucy(output)\n",
    "        print(x.shape)\n",
    "        #print(output.shape)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        #print(output[j].squeeze().shape)\n",
    "        #print(output.shape)\n",
    "        expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "        #print(expanded_labels)\n",
    "        print(x.shape)\n",
    "        #fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
    "        #for j, ax in enumerate(axes.flat):\n",
    "        #    ax.imshow(output[j].squeeze().cpu(), cmap='gray')  \n",
    "        #    ax.set_title(f\"Label: {expanded_labels[j]}\")\n",
    "        #    ax.axis('off')\n",
    "        #plt.show()\n",
    "\n",
    "        if j >= 2: \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d2410e-0cc5-4a02-baaa-1e7df8874cbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.1188e+00,  5.8007e-01,  4.0919e-02,  8.0725e-01,  4.6354e-01,\n",
      "         2.7517e-01,  1.1201e+00,  6.3135e-01,  3.6666e-01,  1.5494e+00,\n",
      "         8.6487e-01,  4.9778e-01,  2.1088e+00,  1.1658e+00,  6.6601e-01,\n",
      "         2.7702e+00,  1.5074e+00,  8.5088e-01,  8.9115e-01,  1.7766e-01,\n",
      "         9.4249e-02,  2.2064e-02,  1.1215e+00,  2.5165e-01,  1.1359e-01,\n",
      "         2.4344e-02,  1.0981e+00,  2.8786e-01,  1.5740e-01,  3.1814e-02,\n",
      "         7.8608e-01,  1.2580e-01,  5.4116e-02,  1.6262e-02,  8.2671e-02,\n",
      "         7.1143e-02,  5.8561e-02,  1.0913e-01,  6.6748e-02,  7.8489e-02,\n",
      "         9.8389e-01,  2.2002e-01,  1.7930e-01,  1.7653e-02,  1.2308e+00,\n",
      "         2.2618e-01,  1.4270e-01,  4.1203e-02,  1.3359e+00,  3.4356e-01,\n",
      "         2.3872e-01,  4.1744e-02,  7.7688e-01,  1.0423e-01,  7.0237e-02,\n",
      "         2.0502e-02,  7.7644e-02,  6.6072e-02,  5.6152e-02,  1.1097e-01,\n",
      "         5.7315e-02,  7.8774e-02,  1.8669e+00,  5.3379e-01,  4.6156e-01,\n",
      "         3.9432e-02,  2.0226e+00,  4.0017e-01,  2.9430e-01,  6.3391e-02,\n",
      "         2.2057e+00,  6.1311e-01,  4.9117e-01,  5.4495e-02,  1.2308e+00,\n",
      "         1.9393e-01,  1.5546e-01,  4.2262e-02,  1.6980e-01,  1.4249e-01,\n",
      "         1.3058e-01,  2.0147e-01,  1.1008e-01,  1.3926e-01,  4.2752e+00,\n",
      "         1.4232e+00,  0.0000e+00,  3.5795e+00,  8.9067e-01,  4.0659e-02,\n",
      "         4.2739e+00,  1.3952e+00,  0.0000e+00,  2.5371e+00,  4.7381e-01,\n",
      "         2.7607e-02,  4.5372e-01,  4.4438e-01,  3.6420e-01,  4.6966e-01,\n",
      "         2.7736e-01,  3.3718e-01,  7.2431e-02,  8.2306e-03, -4.7954e-02,\n",
      "         1.0964e-01,  6.7320e-03,  6.8980e-02, -2.3343e-02,  2.0828e-02,\n",
      "        -7.5810e-03,  1.6032e-02, -3.3204e-02,  4.9235e-02, -3.3312e-03,\n",
      "         8.6448e-03,  7.4932e-02,  7.7185e-03, -5.5434e-02,  1.1542e-01,\n",
      "         4.6549e-03,  7.0476e-02, -5.0742e-02,  5.0083e-02, -1.0465e-02,\n",
      "        -1.0844e-02, -4.9606e-02,  5.5369e-02, -1.6496e-03,  1.6912e-02,\n",
      "         1.6648e-01,  1.0942e-02, -1.3490e-01,  2.0622e-01, -4.0412e-03,\n",
      "         1.2097e-01, -8.6287e-02,  8.2991e-02, -3.9532e-02,  4.5146e-02,\n",
      "        -5.4459e-02,  2.7782e-03,  1.6246e-02,  2.3101e-02,  4.4320e-01,\n",
      "         2.0023e-02, -3.4113e-01,  4.2712e-01, -1.4750e-02,  3.0551e-01],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.1188e+00,  5.8007e-01,  4.0919e-02,  8.0725e-01,  4.6354e-01,\n",
      "         2.7517e-01,  1.1201e+00,  6.3135e-01,  3.6666e-01,  1.5494e+00,\n",
      "         8.6487e-01,  4.9778e-01,  2.1088e+00,  1.1658e+00,  6.6601e-01,\n",
      "         2.7702e+00,  1.5074e+00,  8.5088e-01,  8.9115e-01,  1.7766e-01,\n",
      "         9.4249e-02,  2.2064e-02,  1.1215e+00,  2.5165e-01,  1.1359e-01,\n",
      "         2.4344e-02,  1.0981e+00,  2.8786e-01,  1.5740e-01,  3.1814e-02,\n",
      "         7.8608e-01,  1.2580e-01,  5.4116e-02,  1.6262e-02,  8.2671e-02,\n",
      "         7.1143e-02,  5.8561e-02,  1.0913e-01,  6.6748e-02,  7.8489e-02,\n",
      "         9.8389e-01,  2.2002e-01,  1.7930e-01,  1.7653e-02,  1.2308e+00,\n",
      "         2.2618e-01,  1.4270e-01,  4.1203e-02,  1.3359e+00,  3.4356e-01,\n",
      "         2.3872e-01,  4.1744e-02,  7.7688e-01,  1.0423e-01,  7.0237e-02,\n",
      "         2.0502e-02,  7.7644e-02,  6.6072e-02,  5.6152e-02,  1.1097e-01,\n",
      "         5.7315e-02,  7.8774e-02,  1.8669e+00,  5.3379e-01,  4.6156e-01,\n",
      "         3.9432e-02,  2.0226e+00,  4.0017e-01,  2.9430e-01,  6.3391e-02,\n",
      "         2.2057e+00,  6.1311e-01,  4.9117e-01,  5.4495e-02,  1.2308e+00,\n",
      "         1.9393e-01,  1.5546e-01,  4.2262e-02,  1.6980e-01,  1.4249e-01,\n",
      "         1.3058e-01,  2.0147e-01,  1.1008e-01,  1.3926e-01,  4.2752e+00,\n",
      "         1.4232e+00,  0.0000e+00,  3.5795e+00,  8.9067e-01,  4.0659e-02,\n",
      "         4.2739e+00,  1.3952e+00,  0.0000e+00,  2.5371e+00,  4.7381e-01,\n",
      "         2.7607e-02,  4.5372e-01,  4.4438e-01,  3.6420e-01,  4.6966e-01,\n",
      "         2.7736e-01,  3.3718e-01,  7.2431e-02,  8.2306e-03, -4.7954e-02,\n",
      "         1.0964e-01,  6.7320e-03,  6.8980e-02, -2.3343e-02,  2.0828e-02,\n",
      "        -7.5810e-03,  1.6032e-02, -3.3204e-02,  4.9235e-02, -3.3312e-03,\n",
      "         8.6448e-03,  7.4932e-02,  7.7185e-03, -5.5434e-02,  1.1542e-01,\n",
      "         4.6549e-03,  7.0476e-02, -5.0742e-02,  5.0083e-02, -1.0465e-02,\n",
      "        -1.0844e-02, -4.9606e-02,  5.5369e-02, -1.6496e-03,  1.6912e-02,\n",
      "         1.6648e-01,  1.0942e-02, -1.3490e-01,  2.0622e-01, -4.0412e-03,\n",
      "         1.2097e-01, -8.6287e-02,  8.2991e-02, -3.9532e-02,  4.5146e-02,\n",
      "        -5.4459e-02,  2.7782e-03,  1.6246e-02,  2.3101e-02,  4.4320e-01,\n",
      "         2.0023e-02, -3.4113e-01,  4.2712e-01, -1.4750e-02,  3.0551e-01],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m = x[1,:]\n",
    "print(m)\n",
    "m2 = x[2,:]\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734afc68-1d85-49f4-bf20-61135f42fd73",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0137e+00,  5.0777e-01,  3.4296e-03,  7.1583e-01,  3.6377e-01,\n",
      "         1.8610e-01,  1.0034e+00,  5.0855e-01,  2.5958e-01,  1.3926e+00,\n",
      "         7.0297e-01,  3.5766e-01,  1.8966e+00,  9.5220e-01,  4.8329e-01,\n",
      "         2.4796e+00,  1.2364e+00,  6.2878e-01,  3.3167e-01,  6.7510e-02,\n",
      "         5.7115e-02,  1.4529e-03,  3.5113e-01,  3.1489e-02,  2.6969e-02,\n",
      "         2.2738e-03,  3.9437e-01,  7.7429e-02,  6.5110e-02,  2.1895e-03,\n",
      "         2.9339e-01,  2.4840e-02,  2.1070e-02,  2.0169e-03,  1.6121e-02,\n",
      "         9.0356e-03,  1.4963e-02,  1.8486e-02,  1.2351e-02,  1.6867e-02,\n",
      "         7.5064e-01,  1.6671e-01,  1.4823e-01,  7.4956e-03,  7.5115e-01,\n",
      "         8.4568e-02,  6.5529e-02,  1.6544e-02,  8.6061e-01,  1.9042e-01,\n",
      "         1.6191e-01,  1.1981e-02,  6.3577e-01,  6.6853e-02,  5.5573e-02,\n",
      "         1.3987e-02,  4.3055e-02,  2.9400e-02,  3.9496e-02,  4.8488e-02,\n",
      "         3.2325e-02,  4.4052e-02,  1.6505e+00,  4.5024e-01,  3.9269e-01,\n",
      "         1.1711e-02,  1.4021e+00,  1.9038e-01,  1.4579e-01,  2.7314e-02,\n",
      "         1.7296e+00,  4.7198e-01,  4.0971e-01,  1.2069e-02,  1.2770e+00,\n",
      "         1.6969e-01,  1.3504e-01,  2.8142e-02,  1.1014e-01,  8.4068e-02,\n",
      "         1.0717e-01,  1.1749e-01,  8.0504e-02,  1.1103e-01,  3.4736e+00,\n",
      "         1.1357e+00,  0.0000e+00,  2.4658e+00,  4.2084e-01,  2.7003e-02,\n",
      "         3.5851e+00,  1.1670e+00,  0.0000e+00,  2.3343e+00,  3.9672e-01,\n",
      "         1.8314e-02,  2.8101e-01,  2.5698e-01,  2.7378e-01,  2.8333e-01,\n",
      "         1.8763e-01,  2.7858e-01,  1.7286e-02,  4.5127e-04, -1.5966e-02,\n",
      "         2.0103e-02,  6.4616e-04,  1.8096e-02, -3.9021e-03,  2.0985e-02,\n",
      "        -2.5510e-03,  9.1425e-03, -9.2852e-04,  2.3548e-02, -3.9123e-04,\n",
      "         6.8866e-03,  4.4371e-02,  1.1275e-03, -4.0447e-02,  5.1037e-02,\n",
      "         1.5009e-03,  4.6201e-02,  1.4221e-03,  5.0160e-02, -8.2508e-03,\n",
      "         2.5123e-02, -9.6008e-03,  6.2329e-02,  5.5234e-03,  1.6807e-02,\n",
      "         1.1632e-01,  1.2125e-03, -1.1124e-01,  1.1885e-01,  2.2885e-03,\n",
      "         1.1525e-01,  1.9528e-02,  6.4578e-02,  1.2410e-02,  1.9995e-02,\n",
      "        -1.1778e-02,  6.7335e-02,  8.1964e-03,  1.1008e-02,  2.8244e-01,\n",
      "        -3.4008e-03, -2.8529e-01,  2.9179e-01,  1.4354e-03,  2.8130e-01],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "l = m\n",
    "l2 = m2\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1486e-8741-4643-8dea-8c6ea48f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "similarity12 = cos(m, m2).item()\n",
    "similarity13 = cos(m, l).item()\n",
    "similarity23 = cos(m2, l).item()\n",
    "print(similarity12)\n",
    "print(similarity13)\n",
    "print(similarity23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "668e398d-3fba-4531-a115-d449b5b2ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3814940452575684\n",
      "2.582104444503784\n",
      "3.4768214225769043\n"
     ]
    }
   ],
   "source": [
    "distance12 = torch.norm(m-m2).item()\n",
    "distance13 = torch.norm(m-l).item()\n",
    "distance23 = torch.norm(m2-l).item()\n",
    "print(distance12)\n",
    "print(distance13)\n",
    "print(distance23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c1100-3cf3-4dac-9a1e-013f07d52aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = m\n",
    "array2 = m2\n",
    "array3 = l\n",
    "\n",
    "# Combine tensors into a single tensor\n",
    "combined = torch.stack([array1, array2, array3])\n",
    "print(combined)\n",
    "\n",
    "# Normalize\n",
    "mean = torch.mean(combined, dim=0)\n",
    "std = torch.std(combined, dim=0)\n",
    "std[std == 0] = 1  # prevent division by zero\n",
    "scaled_arrays = (combined - mean) / std\n",
    "#print(scaled_arrays)\n",
    "m = scaled_arrays[0,:]\n",
    "m2= scaled_arrays[1,:]\n",
    "l = scaled_arrays[2,:]\n",
    "\n",
    "distance12 = torch.norm(m-m2).item()\n",
    "distance13 = torch.norm(m-l).item()\n",
    "distance23 = torch.norm(m2-l).item()\n",
    "print(distance12)\n",
    "print(distance13)\n",
    "print(distance23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34997e72-0c9c-426b-ad58-4ac81e748544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de6860-43e3-49ca-bbf5-00c492f2d669",
   "metadata": {},
   "source": [
    "## Test Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f1091-0831-4a35-b4c2-a9c593fd16e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478886-9627-479f-8c88-4ad8da1b253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextureClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(150, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b2925-1429-4bb8-918f-a0df9c10da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextureClassifier(47)  # replace `number_of_classes` with your actual number\n",
    "model = model.to(device)\n",
    "statnet_model = sne.StatNetEncoder(img_size=(crop_size,crop_size),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_stats=num_stats,\n",
    "                                   device=device)\n",
    "statnet_model.to(device)\n",
    "stat_labels = statnet_model.getsstatlabels(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b58f87-dacf-4772-a3cc-86f89212178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second try\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "# Create data indices for training and validation splits\n",
    "dataset_size = len(dtd_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dtd_dataset, batch_size=batch_size, sampler=train_sampler, shuffle = False)\n",
    "validation_loader = DataLoader(dtd_dataset, batch_size=batch_size, sampler=valid_sampler, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e50eb-5456-4c5e-8e44-02b29d7fe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in train_loader:\n",
    "    print(x.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffd36-2761-4226-ab51-a9206829074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, (texture_batch, labels) in enumerate(train_loader):\n",
    "        # Move data to device and flatten it\n",
    "        output = texture_batch.to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        \n",
    "        # Expand labels to account for crops\n",
    "        expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "        \n",
    "        # Skip if batch is smaller than usual\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        if current_batch_size < batch_size:\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        stats = statnet_model.encoder(output)\n",
    "        print(stats[1,:])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(stats)\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs, expanded_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += expanded_labels.size(0)\n",
    "        correct += (predicted == expanded_labels).sum().item()\n",
    "\n",
    "    # Print statistics for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Accuracy: {100 * correct / total}\")\n",
    "\n",
    "\n",
    "    # Initialize variables to keep track of correct predictions and total predictions\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Disable gradient computation to speed up validation\n",
    "    with torch.no_grad():\n",
    "        for j, (texture_batch, labels) in enumerate(validation_loader):\n",
    "            # Prepare data\n",
    "            output = texture_batch.to(device)\n",
    "            output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "            expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "\n",
    "            # Skip if the batch is not full-sized\n",
    "            current_batch_size = output.shape[0] // num_crops\n",
    "            if current_batch_size < batch_size:\n",
    "                continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "            # Forward pass through the models\n",
    "            stats = statnet_model.encoder(output)\n",
    "            outputs = model(stats)\n",
    "\n",
    "            # Calculate predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Update counters\n",
    "            total_correct += (predicted == expanded_labels).sum().item()\n",
    "            total_predictions += expanded_labels.size(0)\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    accuracy = 100 * total_correct / total_predictions\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "    statnet_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cac1f-098b-4e67-be13-818c250fb3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ab3f9-94e8-41f3-89b8-62df2ba4a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop\n",
    "    for j, (texture_batch, labels) in enumerate(train_loader):\n",
    "        output = texture_batch[0].to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        #apply random permutation\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        #if current_batch_size < batch_size:\n",
    "            #continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "\n",
    "        # Assuming stats is a PyTorch tensor of shape (batch_size, 150)\n",
    "        #print(output.shape)\n",
    "        stats = statnet_model.encoder(output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(stats.shape)\n",
    "        outputs = model(stats)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    with torch.no_grad():\n",
    "           for texture_batch, labels in validation_loader:\n",
    "            output = texture_batch.to(device)\n",
    "            labels = torch.tensor([label for label in labels for _ in range(num_crops)]).to(device)\n",
    "            current_batch_size = output.shape[0] // num_crops\n",
    "\n",
    "            #if current_batch_size < batch_size:\n",
    "                #continue  # Skip this batch if it's smaller than the usual batch size\n",
    "            \n",
    "            stats = statnet_model.encoder(output)\n",
    "            outputs = model(stats)\n",
    "            loss = criterion(outputs, texture_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += texture_labels.size(0)\n",
    "            print(total)\n",
    "            correct += (predicted == texture_labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551e756-4a61-4650-b561-701e6d7d8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    for j, texture_batch in enumerate(dtd_dataloader):\n",
    "        #grab texture batch and generate matching labels\n",
    "        output = texture_batch[0].to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        \n",
    "        if current_batch_size < batch_size:\n",
    "            continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "\n",
    "        #calculate stats\n",
    "        stats_vector = statnet_model.forward(output) #statsvector torch.Size([batchsize*crops, 50])\n",
    "        #print(texture_labels.shape)\n",
    "        #print(stats_vector.shape)\n",
    "        \n",
    "        #loss definitions\n",
    "        representation_loss = loss_func_contrastive(stats_vector, texture_labels)\n",
    "        sparsity_loss = statnet_model.sparse_loss()\n",
    "        loss = (representation_loss + \n",
    "                (sparsity_penalty * sparsity_loss))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "\n",
    "        print('*',end='')\n",
    "        #training_loss.append(loss.item())      \n",
    "        if(j%30==0):\n",
    "            print(loss.item())\n",
    "        if(j==100):\n",
    "            break;\n",
    "        training_representation_loss.append(representation_loss.item())\n",
    "        training_sparsity_loss.append(loss.item())    \n",
    "        training_loss.append(loss.item())\n",
    "    num_total_epochs = num_total_epochs + 1\n",
    "    print(f'Finished Epoch {i}. Loss at {loss}.')\n",
    "    #print('Initial Weights:', statnet_model._w)    \n",
    "    if(i%10==0):\n",
    "        compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "        plt.imshow(compressor_mat)\n",
    "        plt.colorbar() \n",
    "        plt.show()\n",
    "    torch.save(\n",
    "            {'state_dict': statnet_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_loss':training_loss,\n",
    "            'last_output':output,\n",
    "            'last_statvec':stats_vector},\n",
    "            f'./model_checkpoint_epoch_{i}.pth')\n",
    "print('All Done!')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047022-f77a-4eb7-955c-3889f33335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef973f0-65ab-44a2-b81f-eca0581488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(compressor_mat)\n",
    "plt.colorbar()\n",
    "print(compressor_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805de811-fdbf-48ab-baeb-517e15242ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_sums = np.sum(np.abs(compressor_mat),axis=0) #how weighted is each of the 150 stats?\n",
    "stat_index_array = np.argsort(stat_sums)[::-1] #get their importance order\n",
    "\n",
    "ordered_stat_sums = stat_sums[stat_index_array]\n",
    "ordered_stat_labels = np.array(stat_labels,dtype=object)[stat_index_array]\n",
    "ordered_stats_labels_pretty = [' '.join([''.join(str(item)) for item in row if item not in ['',None]]) for row in ordered_stat_labels]\n",
    "\n",
    "n=25\n",
    "print(f'Most Important {n} Stats:')\n",
    "for i in range(n):\n",
    "    print(ordered_stat_sums[i],ordered_stats_labels_pretty[i])\n",
    "\n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[:n]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "top_stats = ordered_stats_labels_pretty[:n]\n",
    "ax.set_yticks(np.arange(len(top_stats)), labels=top_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    \n",
    "    \n",
    "#plot most important stats for contrastive learning\n",
    "#plt.imshow(compressor_mat[:,stat_index_array[:n]].T)\n",
    "\n",
    "print(f'Least Important {n} Stats:')\n",
    "for i in range(1,n+1):\n",
    "    print(ordered_stat_sums[-i],ordered_stats_labels_pretty[-i])\n",
    "    \n",
    "    \n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[-n:]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "bot_stats = ordered_stats_labels_pretty[-n:][::-1]\n",
    "ax.set_yticks(np.arange(len(bot_stats)), labels=bot_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317dc4c-4cbd-4429-b13d-deb0e2ea987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?list.insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62797e26-2dbc-4230-9f05-91c2d009bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.8/site-packages/')\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "#texture_labels\n",
    "#output\n",
    "#calculate stats\n",
    "stats_vector = statnet_model.forward(output)\n",
    "print(stats_vector.shape)\n",
    "\n",
    "X = stats_vector  # shape (n_samples, d)\n",
    "X_emb = TSNE(n_components=2, perplexity=10, n_iter=10000, verbose=True).fit_transform(X)  # returns shape (n_samples, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d92a0e-05c9-43de-b300-6081a03d1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "#texture_labels\n",
    "#output\n",
    "#calculate stats\n",
    "stats_vector = statnet_model.forward(output)\n",
    "print(stats_vector.shape)\n",
    "\n",
    "X = stats_vector  # shape (n_samples, d)\n",
    "X_emb = TSNE(n_components=2, perplexity=10, n_iter=10000, verbose=True).fit_transform(X)  # returns shape (n_samples, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19995b08-a1c6-43d6-b9c1-365be30a9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plt.legend(handles=scatter.legend_elements()[0], labels=texture_labels, title=\"Textures\")\n",
    "texture_label_list = texture_labels.numpy()\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#scatter = ax.scatter(X_emb[:,0],X_emb[:,1],c=texture_label_list,label=texture_label_list)\n",
    "#plt.legend()#handles=scatter.legend_elements()[0], labels=texture_label_list)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.legend(handles=scatter.legend_elements()[0],title=\"texture\")\n",
    "\n",
    "#handles = [plt.plot([],color=sc.get_cmap()(sc.norm(c)),ls=\"\", marker=\"o\")[0] for c,l in clset ]\n",
    "#labels = [l for c,l in clset]\n",
    "#ax.legend(texture_label_list)\n",
    "\n",
    "unique_textures = set(texture_label_list)\n",
    "\n",
    "#fix, ax = plt.subplots()\n",
    "plt.figure(figsize=(6,6))\n",
    "for i, texture in enumerate(unique_textures):\n",
    "    texture_index = np.where(texture_label_list == texture)[0]\n",
    "    texture_index = np.where(texture_label_list==i)[0]\n",
    "    plt.scatter(X_emb[texture_index,0],X_emb[texture_index,1],label=i)\n",
    "plt.legend()\n",
    "plt.title(f'T-sne embeddings: {num_crops} crops, {batch_size} textures')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, label in enumerate(texture_label_list):\n",
    "    plt.annotate(label, (X_emb[i,0],X_emb[i,1] + 0.2))\n",
    "\n",
    "#for i, txt in enumerate(texture_label_list):\n",
    "#    plt.annotate(texture_label_list, (X_emb[:,0],X_emb[:,1]))\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e10161-e890-4977-af82-c0cae58a4105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
