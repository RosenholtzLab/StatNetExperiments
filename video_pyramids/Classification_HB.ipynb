{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac877ae4-68f4-4521-ba5d-b8145777e4b2",
   "metadata": {},
   "source": [
    "## Classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105dbf5b-47dc-4889-98b8-04bffd49e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tsne_torch import TorchTSNE\n",
    "import numpy as np\n",
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.9/site-packages/')\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PooledStatisticsMetamers/poolstatmetamer/')\n",
    "import utils.statnetencoder as sne\n",
    "import importlib\n",
    "import imp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\chris\\Documents\\MIT\\Statistics_analysis_code\\PyTorchSteerablePyramid')\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PyTorchSteerablePyramid/')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f5089e-83cc-4ece-807b-9fa932d923b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd_folder = '/home/gridsan/ckoevesdi/data/dtd_torch/dtd/dtd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b8918-56d9-44b3-ba83-f1e749a1bc4b",
   "metadata": {},
   "source": [
    "## Creation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5af0fec-2c2b-4e41-9003-52b15ee69478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(size=300),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # This will also convert the image from [0, 255] to [0.0, 1.0]\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "class_dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/dtd_torch/', split='val', \n",
    "                                             partition=10, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "# Define the batch size (Change this based on your requirements)\n",
    "batch_size = 1\n",
    "\n",
    "# Create a DataLoader\n",
    "classification_dataloader = DataLoader(\n",
    "    class_dtd_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(class_dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc94447-c61a-4d22-9680-c4930d62fe35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(False):\n",
    "    for j, (texture_batch, labels) in enumerate(classification_dataloader):\n",
    "            # Move data to device and flatten it\n",
    "            print(texture_batch)\n",
    "            print(labels)\n",
    "            output = texture_batch.to(device)\n",
    "            print(output.shape)\n",
    "            for i in range(batch_size):\n",
    "                    plt.figure()\n",
    "                    plt.imshow(tensor2pil_transform(output[0,0,:,:]),cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            if(j>3):\n",
    "                break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbe42b2-3005-49ad-aab5-e9e834a1123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solver: pyramid=UBBBBL_6 pooling=WholeImagePooling()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import utils.brucenet as bn\n",
    "brucy = bn.BruceNet(pooling_region_size=1e20, pyramid_params=False, dummy_img = torch.zeros(20,\n",
    "                                     1,\n",
    "                                     300,\n",
    "                                     300)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1110aaad-1040-427b-871f-1425e47184a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_statistics = []\n",
    "\n",
    "for j, (texture_batch, labels) in enumerate(classification_dataloader):\n",
    "    # Assuming texture_batch has a shape of [20, 5, 1, 128, 128]\n",
    "    #print(texture_batch.shape)\n",
    "    output = texture_batch.to(device)\n",
    "    output = torch.stack([output, output]).squeeze(1)\n",
    "    #print(output.shape)\n",
    "    #print(labels)\n",
    "    statistics = brucy(output)  # This should output a tensor of shape [2, 150]\n",
    "    statistics = statistics[0,:]\n",
    "    #print(statistics[0,:])\n",
    "    # You can now store these statistics, along with the label and original image index\n",
    "    all_statistics.append({\n",
    "        'statistics': statistics.cpu().numpy(),\n",
    "        #'label': labels[i].item(),\n",
    "        'original_image_index': labels.cpu().numpy()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e654f0ed-86a2-4ff3-afbb-456ec87ca205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'statistics': array([ 6.30244672e-01,  1.54877827e-01,  1.60244368e-02,  2.18463764e-01,\n",
      "        1.15038104e-01,  6.33021072e-02,  3.04496050e-01,  1.57857299e-01,\n",
      "        8.57136175e-02,  4.17947620e-01,  2.09199965e-01,  1.09768219e-01,\n",
      "        5.69129884e-01,  2.71880656e-01,  1.35975912e-01,  7.70545185e-01,\n",
      "        3.49232107e-01,  1.64992988e-01,  2.74513751e-01,  9.54570025e-02,\n",
      "        1.55887395e-01,  2.83173155e-02,  4.13577020e-01,  1.85625806e-01,\n",
      "        2.75265217e-01,  3.91351394e-02,  3.89543116e-01,  1.73685029e-01,\n",
      "        2.80285537e-01,  4.92304303e-02,  2.20321164e-01,  4.96202409e-02,\n",
      "        9.71030742e-02,  1.49331884e-02,  1.03997104e-01,  7.66281188e-02,\n",
      "        5.25283813e-02,  1.51419520e-01,  7.26732090e-02,  7.62581602e-02,\n",
      "        6.14765644e-01,  2.67693371e-01,  3.80115896e-01,  1.54476583e-01,\n",
      "        8.28454852e-01,  4.64545786e-01,  4.82536107e-01,  3.10085833e-01,\n",
      "        8.57578456e-01,  4.99076843e-01,  6.17457628e-01,  2.92146415e-01,\n",
      "        5.58817744e-01,  2.02339277e-01,  3.12645555e-01,  1.42545819e-01,\n",
      "        2.83847988e-01,  2.46530175e-01,  1.83895081e-01,  4.09905851e-01,\n",
      "        2.33777300e-01,  2.65492737e-01,  1.10776329e+00,  5.54759502e-01,\n",
      "        7.01651931e-01,  3.33010703e-01,  1.15965879e+00,  5.67065060e-01,\n",
      "        5.68796635e-01,  4.00666416e-01,  1.39094234e+00,  8.35574448e-01,\n",
      "        1.05352271e+00,  5.30179977e-01,  1.11174452e+00,  5.18459141e-01,\n",
      "        6.03801370e-01,  3.91855061e-01,  4.43560392e-01,  4.47258890e-01,\n",
      "        4.21369046e-01,  5.76056182e-01,  4.20794904e-01,  5.51773846e-01,\n",
      "        1.71779454e+00,  9.01033938e-01,  3.01534295e-01,  1.60006690e+00,\n",
      "        6.67788267e-01,  2.54340798e-01,  2.20675516e+00,  1.35712552e+00,\n",
      "        6.03361011e-01,  1.74615037e+00,  8.11608434e-01,  4.14425135e-01,\n",
      "        6.10652328e-01,  6.84565663e-01,  6.83943987e-01,  7.98200250e-01,\n",
      "        5.97680092e-01,  8.79494250e-01,  1.96597561e-01,  3.38504165e-02,\n",
      "       -7.74244666e-02,  3.04741681e-01,  1.96768865e-02,  1.34549350e-01,\n",
      "       -7.45778065e-03, -2.33709328e-02,  2.86485301e-03, -4.50995527e-02,\n",
      "        8.02783854e-03, -2.38492098e-02,  8.48011021e-03, -1.54628158e-02,\n",
      "        4.96061742e-01,  6.60895035e-02, -2.68940151e-01,  7.94261336e-01,\n",
      "        5.66747785e-02,  4.68201667e-01, -8.85001663e-03, -3.26001532e-02,\n",
      "        5.31644933e-02, -1.00546613e-01, -3.64056453e-02, -1.05032898e-01,\n",
      "        3.79925445e-02, -9.27378424e-03,  7.57442117e-01,  1.09320031e-02,\n",
      "       -6.87375367e-01,  1.04871833e+00,  6.71965554e-02,  1.00882196e+00,\n",
      "        4.00532316e-03, -1.06748335e-01, -4.37850095e-02, -7.36891106e-02,\n",
      "        1.01674519e-01, -3.27944487e-01,  9.99302254e-04, -9.39311385e-02,\n",
      "        1.02657735e+00, -4.92851585e-02, -1.18153965e+00,  1.52354050e+00,\n",
      "        1.24341145e-01,  1.59822595e+00], dtype=float32), 'original_image_index': array([36])}\n"
     ]
    }
   ],
   "source": [
    "print(all_statistics[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaec2070-a022-4fe7-bb69-b1f4bf7ea809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(939, 150)\n",
      "tensor([ 0.6732,  0.5422,  0.3349,  0.5369,  0.3981,  0.2791,  0.5205,  0.3594,\n",
      "         0.2196,  0.4876,  0.2863,  0.1111,  0.4892,  0.2805,  0.0941,  0.5167,\n",
      "         0.3269,  0.1496,  1.4103,  0.8753,  0.9940,  0.7747,  1.4585,  0.9613,\n",
      "         1.1522,  0.8031,  0.8904,  0.3291,  0.5801,  0.3489,  1.0333,  0.4893,\n",
      "         0.7976,  0.4762,  1.0264,  0.7965,  0.7382,  0.6431,  0.7622,  0.4163,\n",
      "         1.5174,  0.9432,  0.5800,  0.8578,  1.5895,  1.0989,  0.5598,  0.8178,\n",
      "         1.3301,  0.7296,  0.3289,  0.6552,  1.5067,  0.9389,  0.4592,  0.7414,\n",
      "         1.1528,  1.3396,  1.0931,  1.0230,  1.1643,  0.9132,  0.6266,  0.1804,\n",
      "         0.0521, -0.0978,  0.4873,  0.0607, -0.1757, -0.1980,  0.3323, -0.0611,\n",
      "        -0.1530, -0.2909,  0.4535,  0.0305, -0.1991, -0.1392,  0.1664,  0.2906,\n",
      "         0.1492,  0.0356,  0.1046,  0.0126, -0.0792, -0.1445, -0.3838, -0.2528,\n",
      "        -0.3743, -0.5031, -0.2109, -0.2669, -0.4068, -0.3035, -0.3936, -0.5036,\n",
      "        -0.2767, -0.3663, -0.2864, -0.3656, -0.3877, -0.3836,  1.0633,  1.0908,\n",
      "        -0.7089,  0.5758, -0.5170,  0.2856,  0.6560,  0.1386,  2.0479,  0.1901,\n",
      "        -0.7862, -0.1210, -0.4535,  0.2079,  1.0343,  0.0237, -0.9921,  0.8965,\n",
      "        -0.1847,  0.7471,  0.9269, -0.2532, -0.5387, -0.1171,  0.5319, -0.1236,\n",
      "         1.7968, -0.0691,  0.1453,  0.0224, -0.1416, -0.0215, -0.2217, -0.0544,\n",
      "         0.1003, -0.1079,  0.4746, -0.2401,  0.1644, -0.5906, -0.2291, -0.2707,\n",
      "        -0.2009,  0.0120,  0.2303, -0.3370, -0.0576, -0.3478])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Convert the list of dictionaries to a NumPy array or PyTorch tensor\n",
    "statistics_array = np.array([item['statistics'] for item in all_statistics])\n",
    "labels_array = np.array([item['original_image_index'] for item in all_statistics])\n",
    "print(statistics_array.shape)\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(statistics_array)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "statistics_tensor = torch.tensor(scaled_array, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "print(statistics_tensor[2, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdb00df-edb0-4d9c-b63d-fd918505633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6732, 0.5422, 0.3349, 0.5369, 0.3981, 0.2791, 0.5205, 0.3594, 0.2196,\n",
      "        0.4876, 0.2863, 0.1111, 0.4892, 0.2805, 0.0941, 0.5167, 0.3269, 0.1496,\n",
      "        1.4103, 0.8753, 0.9940, 0.7747, 1.4585, 0.9613, 1.1522, 0.8031, 0.8904,\n",
      "        0.3291, 0.5801, 0.3489, 1.0333, 0.4893, 0.7976, 0.4762, 1.0264, 0.7965,\n",
      "        0.7382, 0.6431, 0.7622, 0.4163, 1.5174, 0.9432, 0.5800, 0.8578, 1.5895,\n",
      "        1.0989, 0.5598, 0.8178, 1.3301])\n"
     ]
    }
   ],
   "source": [
    "print(statistics_tensor[2, :49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6af6af8-1661-4f4d-b299-f55456b3c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatisticsDataset(Dataset):\n",
    "    def __init__(self, statistics_tensor, labels_tensor):\n",
    "        self.statistics = statistics_tensor\n",
    "        self.labels = labels_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.statistics[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset object\n",
    "statistics_dataset = StatisticsDataset(statistics_tensor, labels_tensor)\n",
    "\n",
    "batches = 80\n",
    "class_statistics_dataloader = DataLoader(statistics_dataset, batch_size=batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62602c32-9dcf-4900-8173-d9811326252f",
   "metadata": {},
   "source": [
    "Checking does it make sense to even try? Intra-class variation statistics wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caa1a5ae-af3d-425c-b485-03559c70e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.405881881713867\n",
      "22.376495361328125\n",
      "8.1549654006958\n"
     ]
    }
   ],
   "source": [
    "m = statistics_tensor[14]\n",
    "m2 = statistics_tensor[5]\n",
    "l = statistics_tensor[90]\n",
    "\n",
    "distance12 = torch.norm(m-m2).item()\n",
    "distance13 = torch.norm(m-l).item()\n",
    "distance23 = torch.norm(m2-l).item()\n",
    "print(distance12)\n",
    "print(distance13)\n",
    "print(distance23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfff1463-01b6-4837-9052-e9ac9abcb898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.455707550048828\n",
      "8.435786247253418\n",
      "3.005241870880127\n"
     ]
    }
   ],
   "source": [
    "m = statistics_tensor[14, :49]\n",
    "m2 = statistics_tensor[5, :49]\n",
    "l = statistics_tensor[210, :49]\n",
    "\n",
    "distance12 = torch.norm(m-m2).item()\n",
    "distance13 = torch.norm(m-l).item()\n",
    "distance23 = torch.norm(m2-l).item()\n",
    "print(distance12)\n",
    "print(distance13)\n",
    "print(distance23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b3b45-cd58-460f-93ee-973442cd79c0",
   "metadata": {},
   "source": [
    "## Implementing new model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1be2978-f6fd-44bf-bbd0-5ae5dab7c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(150, 100)\n",
    "        self.layer2 = nn.Linear(100, 75)\n",
    "        self.layer3 = nn.Linear(75, 50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.base_model = EmbeddingModel()  # The pre-trained feature extractor\n",
    "        self.classifier = nn.Linear(50, num_classes)  # New classification head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0982cc76-2256-4230-8bcb-479419ab11e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTunedModel(\n",
       "  (base_model): EmbeddingModel(\n",
       "    (layer1): Linear(in_features=150, out_features=100, bias=True)\n",
       "    (layer2): Linear(in_features=100, out_features=75, bias=True)\n",
       "    (layer3): Linear(in_features=75, out_features=50, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=50, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the fine-tuned model with 47 classes (replace with the actual number of classes in your case)\n",
    "fine_tuned_model = FineTunedModel(num_classes=47)\n",
    "\n",
    "# Load the pre-trained weights into the base_model part of fine_tuned_model\n",
    "fine_tuned_model.base_model.load_state_dict(torch.load('contrastive_model_HB.pth'))\n",
    "\n",
    "# Move model to the device (CPU or CUDA)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e1ff742-4c75-4734-9db7-ab9426544879",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.5222, Accuracy: 34.17%\n",
      "Epoch [2/100], Loss: 2.3246, Accuracy: 35.02%\n",
      "Epoch [3/100], Loss: 2.6315, Accuracy: 35.60%\n",
      "Epoch [4/100], Loss: 2.3418, Accuracy: 34.97%\n",
      "Epoch [5/100], Loss: 2.3688, Accuracy: 35.18%\n",
      "Epoch [6/100], Loss: 1.9851, Accuracy: 35.39%\n",
      "Epoch [7/100], Loss: 2.1345, Accuracy: 36.14%\n",
      "Epoch [8/100], Loss: 2.4829, Accuracy: 35.76%\n",
      "Epoch [9/100], Loss: 2.2476, Accuracy: 35.76%\n",
      "Epoch [10/100], Loss: 2.1818, Accuracy: 35.66%\n",
      "Epoch [11/100], Loss: 2.1998, Accuracy: 35.92%\n",
      "Epoch [12/100], Loss: 2.7314, Accuracy: 35.87%\n",
      "Epoch [13/100], Loss: 2.3283, Accuracy: 35.98%\n",
      "Epoch [14/100], Loss: 2.6070, Accuracy: 36.35%\n",
      "Epoch [15/100], Loss: 2.2003, Accuracy: 36.56%\n",
      "Epoch [16/100], Loss: 2.4004, Accuracy: 36.30%\n",
      "Epoch [17/100], Loss: 2.4058, Accuracy: 36.14%\n",
      "Epoch [18/100], Loss: 2.2293, Accuracy: 36.72%\n",
      "Epoch [19/100], Loss: 2.4514, Accuracy: 36.72%\n",
      "Epoch [20/100], Loss: 2.1385, Accuracy: 36.72%\n",
      "Epoch [21/100], Loss: 2.7145, Accuracy: 36.56%\n",
      "Epoch [22/100], Loss: 2.5349, Accuracy: 36.40%\n",
      "Epoch [23/100], Loss: 2.5217, Accuracy: 36.77%\n",
      "Epoch [24/100], Loss: 2.1954, Accuracy: 36.93%\n",
      "Epoch [25/100], Loss: 2.3082, Accuracy: 36.93%\n",
      "Epoch [26/100], Loss: 2.2878, Accuracy: 37.25%\n",
      "Epoch [27/100], Loss: 2.3576, Accuracy: 37.25%\n",
      "Epoch [28/100], Loss: 2.4880, Accuracy: 37.52%\n",
      "Epoch [29/100], Loss: 2.1991, Accuracy: 37.73%\n",
      "Epoch [30/100], Loss: 2.3480, Accuracy: 37.52%\n",
      "Epoch [31/100], Loss: 2.2520, Accuracy: 37.68%\n",
      "Epoch [32/100], Loss: 2.2604, Accuracy: 37.95%\n",
      "Epoch [33/100], Loss: 2.4879, Accuracy: 37.84%\n",
      "Epoch [34/100], Loss: 2.2445, Accuracy: 37.73%\n",
      "Epoch [35/100], Loss: 2.7409, Accuracy: 37.73%\n",
      "Epoch [36/100], Loss: 1.6561, Accuracy: 37.68%\n",
      "Epoch [37/100], Loss: 1.9726, Accuracy: 38.42%\n",
      "Epoch [38/100], Loss: 2.1138, Accuracy: 38.32%\n",
      "Epoch [39/100], Loss: 2.1697, Accuracy: 38.48%\n",
      "Epoch [40/100], Loss: 2.0637, Accuracy: 38.27%\n",
      "Epoch [41/100], Loss: 2.1233, Accuracy: 38.74%\n",
      "Epoch [42/100], Loss: 2.5187, Accuracy: 38.96%\n",
      "Epoch [43/100], Loss: 2.4407, Accuracy: 38.96%\n",
      "Epoch [44/100], Loss: 2.5878, Accuracy: 38.85%\n",
      "Epoch [45/100], Loss: 2.3485, Accuracy: 38.90%\n",
      "Epoch [46/100], Loss: 2.6224, Accuracy: 39.38%\n",
      "Epoch [47/100], Loss: 2.4517, Accuracy: 38.90%\n",
      "Epoch [48/100], Loss: 2.4674, Accuracy: 39.33%\n",
      "Epoch [49/100], Loss: 2.4026, Accuracy: 38.85%\n",
      "Epoch [50/100], Loss: 2.2063, Accuracy: 39.12%\n",
      "Epoch [51/100], Loss: 2.0559, Accuracy: 38.96%\n",
      "Epoch [52/100], Loss: 2.1477, Accuracy: 39.44%\n",
      "Epoch [53/100], Loss: 2.4721, Accuracy: 39.17%\n",
      "Epoch [54/100], Loss: 2.1849, Accuracy: 39.81%\n",
      "Epoch [55/100], Loss: 2.0178, Accuracy: 39.91%\n",
      "Epoch [56/100], Loss: 2.2335, Accuracy: 39.76%\n",
      "Epoch [57/100], Loss: 2.1580, Accuracy: 39.91%\n",
      "Epoch [58/100], Loss: 1.9052, Accuracy: 40.07%\n",
      "Epoch [59/100], Loss: 2.2601, Accuracy: 40.29%\n",
      "Epoch [60/100], Loss: 2.0738, Accuracy: 39.65%\n",
      "Epoch [61/100], Loss: 2.4995, Accuracy: 39.91%\n",
      "Epoch [62/100], Loss: 1.9284, Accuracy: 40.18%\n",
      "Epoch [63/100], Loss: 1.8908, Accuracy: 40.87%\n",
      "Epoch [64/100], Loss: 2.3952, Accuracy: 41.09%\n",
      "Epoch [65/100], Loss: 2.4356, Accuracy: 40.50%\n",
      "Epoch [66/100], Loss: 2.0279, Accuracy: 40.66%\n",
      "Epoch [67/100], Loss: 1.9474, Accuracy: 40.87%\n",
      "Epoch [68/100], Loss: 2.4717, Accuracy: 41.19%\n",
      "Epoch [69/100], Loss: 2.6303, Accuracy: 41.09%\n",
      "Epoch [70/100], Loss: 2.3379, Accuracy: 41.51%\n",
      "Epoch [71/100], Loss: 1.9376, Accuracy: 41.46%\n",
      "Epoch [72/100], Loss: 2.1290, Accuracy: 41.25%\n",
      "Epoch [73/100], Loss: 2.2937, Accuracy: 41.46%\n",
      "Epoch [74/100], Loss: 2.0893, Accuracy: 41.51%\n",
      "Epoch [75/100], Loss: 2.5795, Accuracy: 41.56%\n",
      "Epoch [76/100], Loss: 2.1889, Accuracy: 41.88%\n",
      "Epoch [77/100], Loss: 2.0803, Accuracy: 41.46%\n",
      "Epoch [78/100], Loss: 1.9193, Accuracy: 42.31%\n",
      "Epoch [79/100], Loss: 2.3489, Accuracy: 41.83%\n",
      "Epoch [80/100], Loss: 2.3631, Accuracy: 41.78%\n",
      "Epoch [81/100], Loss: 2.3444, Accuracy: 42.15%\n",
      "Epoch [82/100], Loss: 2.0750, Accuracy: 42.36%\n",
      "Epoch [83/100], Loss: 1.7621, Accuracy: 42.26%\n",
      "Epoch [84/100], Loss: 1.9134, Accuracy: 42.58%\n",
      "Epoch [85/100], Loss: 2.0539, Accuracy: 42.31%\n",
      "Epoch [86/100], Loss: 1.9366, Accuracy: 42.58%\n",
      "Epoch [87/100], Loss: 1.9432, Accuracy: 42.36%\n",
      "Epoch [88/100], Loss: 2.1370, Accuracy: 42.63%\n",
      "Epoch [89/100], Loss: 2.2665, Accuracy: 42.74%\n",
      "Epoch [90/100], Loss: 2.2705, Accuracy: 43.05%\n",
      "Epoch [91/100], Loss: 1.8547, Accuracy: 43.00%\n",
      "Epoch [92/100], Loss: 2.1718, Accuracy: 43.37%\n",
      "Epoch [93/100], Loss: 2.4043, Accuracy: 42.79%\n",
      "Epoch [94/100], Loss: 2.0542, Accuracy: 43.27%\n",
      "Epoch [95/100], Loss: 2.4022, Accuracy: 43.11%\n",
      "Epoch [96/100], Loss: 2.2571, Accuracy: 43.37%\n",
      "Epoch [97/100], Loss: 2.3701, Accuracy: 43.53%\n",
      "Epoch [98/100], Loss: 2.1630, Accuracy: 43.69%\n",
      "Epoch [99/100], Loss: 2.2414, Accuracy: 43.48%\n",
      "Epoch [100/100], Loss: 1.9715, Accuracy: 44.60%\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "# Your classification DataLoader\n",
    "# classification_dataloader = DataLoader(your_classification_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define optimizers and loss function\n",
    "optimizer_base = optim.Adam(fine_tuned_model.base_model.parameters(), lr=1e-4)\n",
    "optimizer_classifier = optim.Adam(fine_tuned_model.classifier.parameters(), lr=1e-3)\n",
    "classification_loss = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100 # Number of epochs for fine-tuning\n",
    "\n",
    "# Initialize variables for tracking accuracy\n",
    "total_samples = 0\n",
    "correct_samples = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in class_statistics_dataloader:  \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.squeeze()\n",
    "        #print(y_batch.shape)\n",
    "        # Forward pass\n",
    "        outputs = fine_tuned_model(x_batch)\n",
    "        #print(outputs.shape)\n",
    "        # Compute loss\n",
    "        loss = classification_loss(outputs, y_batch)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_base.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer_base.step()\n",
    "        optimizer_classifier.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += y_batch.size(0)\n",
    "        correct_samples += (predicted == y_batch).sum().item()\n",
    "\n",
    "    # Calculate accuracy for this epoch\n",
    "    epoch_accuracy = 100 * correct_samples / total_samples\n",
    "\n",
    "    # Logging\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    # Reset accuracy tracking variables for the next epoch\n",
    "    total_samples = 0\n",
    "    correct_samples = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b450c4-80aa-4a4f-9038-a85804b2bee8",
   "metadata": {},
   "source": [
    "Create dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9ca4a3-9809-431c-9e38-3de12471b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(size=300),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # This will also convert the image from [0, 255] to [0.0, 1.0]\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "test_dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/dtd_torch/', split='test', \n",
    "                                            partition=10, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "# Define the batch size (Change this based on your requirements)\n",
    "batch_size = 1\n",
    "\n",
    "# Create a DataLoader\n",
    "test_classification_dataloader = DataLoader(\n",
    "    test_dtd_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(test_dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017ea93e-bd7e-42ca-bb8c-8a3cd9f03dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a5c292-5da3-4c4b-a5a3-b2aa68e85e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_statistics = []\n",
    "\n",
    "for j, (texture_batch, labels) in enumerate(test_classification_dataloader):\n",
    "    # Assuming texture_batch has a shape of [20, 5, 1, 128, 128]\n",
    "    #print(texture_batch.shape)\n",
    "    output = texture_batch.to(device)\n",
    "    output = torch.stack([output, output]).squeeze(1)\n",
    "    #print(output.shape)\n",
    "    #print(labels)\n",
    "    statistics = brucy(output)  # This should output a tensor of shape [2, 150]\n",
    "    statistics = statistics[0,:]\n",
    "    #print(statistics[0,:])\n",
    "    # You can now store these statistics, along with the label and original image index\n",
    "    test_all_statistics.append({\n",
    "        'statistics': statistics.cpu().numpy(),\n",
    "        #'label': labels[i].item(),\n",
    "        'original_image_index': labels.cpu().numpy()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d56e2b9-2933-4807-aab9-fb1ed717094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 150)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Convert the list of dictionaries to a NumPy array or PyTorch tensor\n",
    "test_statistics_array = np.array([item['statistics'] for item in test_all_statistics])\n",
    "test_labels_array = np.array([item['original_image_index'] for item in test_all_statistics])\n",
    "print(test_statistics_array.shape)\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "test_scaled_array = scaler.fit_transform(test_statistics_array)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "test_statistics_tensor = torch.tensor(test_scaled_array, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels_array, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468f387e-85f0-4d0f-87f0-f86f0d28de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatisticsDataset(Dataset):\n",
    "    def __init__(self, statistics_tensor, labels_tensor):\n",
    "        self.statistics = statistics_tensor\n",
    "        self.labels = labels_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.statistics[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset object\n",
    "test_statistics_dataset = StatisticsDataset(test_statistics_tensor, test_labels_tensor)\n",
    "\n",
    "batches = 40\n",
    "test_class_statistics_dataloader = DataLoader(test_statistics_dataset, batch_size=batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ded0ce-1a32-4a86-aa59-9f3d993132c9",
   "metadata": {},
   "source": [
    "# Perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b469ad4e-fdb1-4181-9c91-f42a03865896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Initialize variables for tracking accuracy\n",
    "total_samples = 0\n",
    "correct_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62a5ed15-1176-4006-a836-9d64d02f6c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 13.54%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FineTunedModel(\n",
       "  (base_model): EmbeddingModel(\n",
       "    (layer1): Linear(in_features=150, out_features=100, bias=True)\n",
       "    (layer2): Linear(in_features=100, out_features=75, bias=True)\n",
       "    (layer3): Linear(in_features=75, out_features=50, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=50, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.eval()\n",
    "\n",
    "# Initialize variables for tracking accuracy\n",
    "total_samples = 0\n",
    "correct_samples = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_class_statistics_dataloader:  \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = fine_tuned_model(x_batch)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        total_samples += y_batch.size(0)\n",
    "        correct_samples += (predicted == y_batch).sum().item()\n",
    "\n",
    "# Calculate accuracy for the test set\n",
    "test_accuracy = 100 * correct_samples / total_samples\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Set the model back to training mode (optional if you continue training afterwards)\n",
    "fine_tuned_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "127d4660-cf13-4f7c-b889-016c69eb4d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        11\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.38      0.27      0.32        11\n",
      "           6       0.14      0.09      0.11        11\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.12      0.09      0.11        11\n",
      "           9       0.00      0.00      0.00        11\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.20      0.09      0.13        11\n",
      "          12       0.33      0.18      0.24        11\n",
      "          13       0.08      0.45      0.14        11\n",
      "          14       0.00      0.00      0.00        11\n",
      "          15       0.17      0.36      0.24        11\n",
      "          16       0.00      0.00      0.00        11\n",
      "          17       0.00      0.00      0.00        11\n",
      "          18       0.00      0.00      0.00        11\n",
      "          19       0.00      0.00      0.00        11\n",
      "          20       0.00      0.00      0.00        11\n",
      "          21       0.14      0.36      0.21        11\n",
      "          22       0.05      0.09      0.06        11\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.00      0.00      0.00        11\n",
      "          25       0.20      0.09      0.13        11\n",
      "          26       0.00      0.00      0.00        11\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.33      0.18      0.24        11\n",
      "          29       0.06      0.09      0.07        11\n",
      "          30       0.00      0.00      0.00        11\n",
      "          31       0.15      0.27      0.19        11\n",
      "          32       0.00      0.00      0.00        11\n",
      "          33       0.00      0.00      0.00        11\n",
      "          34       0.00      0.00      0.00        11\n",
      "          35       0.00      0.00      0.00        11\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00        11\n",
      "          38       0.06      0.27      0.09        11\n",
      "          39       0.12      0.36      0.19        11\n",
      "          40       0.16      0.73      0.27        11\n",
      "          41       0.00      0.00      0.00        11\n",
      "          42       0.00      0.00      0.00        11\n",
      "          43       0.05      0.18      0.08        11\n",
      "          44       0.16      0.36      0.22        11\n",
      "          45       0.00      0.00      0.00        11\n",
      "          46       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.10       517\n",
      "   macro avg       0.06      0.10      0.06       517\n",
      "weighted avg       0.06      0.10      0.06       517\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 2 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 0]\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Convert to NumPy arrays for easier slicing and indexing\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd68af8-32a1-4d44-a4d3-6d694af35296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b153d-77b2-4d63-bbea-3d9759a0c7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
