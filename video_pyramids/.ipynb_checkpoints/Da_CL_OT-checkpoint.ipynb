{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c9fe52-26af-4089-803e-4406a29432f3",
   "metadata": {},
   "source": [
    "# StaTexNet - Network Encoding Statistics for Textures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc538b7-27ab-46c2-be44-33847dda73a7",
   "metadata": {},
   "source": [
    "## Dependencies & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e95b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:44:59.904549200Z",
     "start_time": "2023-07-27T14:44:59.889457100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.9/site-packages/')\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PooledStatisticsMetamers/poolstatmetamer/')\n",
    "import utils.statnetencoder as sne\n",
    "import importlib\n",
    "import imp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\chris\\Documents\\MIT\\Statistics_analysis_code\\PyTorchSteerablePyramid')\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PyTorchSteerablePyramid/')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb413b8-72ca-4a7a-b98f-d32cc73b5470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:47:05.643243100Z",
     "start_time": "2023-07-27T14:47:05.619221Z"
    }
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "num_epochs = 50\n",
    "batch_size = 20\n",
    "crop_size = 128 \n",
    "num_stats = 50 \n",
    "optimizer_type='adam'\n",
    "#optimizer_type='sgd'\n",
    "learning_rate = 0.001\n",
    "num_crops = 5 #changed this to four\n",
    "\n",
    "multistat_penalty = 0\n",
    "sparsity_penalty = 0.01\n",
    "entropic_penalty = 0\n",
    "\n",
    "#dataset location\n",
    "#dtd_folder = '/gridsan/ckoevesdi/data/dtd_torch/dt/'\n",
    "dtd_folder = 'home/gridsan/ckoevesdi/data/OT/dtd_torch/dtd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806091f-63d7-4819-ad4e-2ef117a7097d",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1482b4a6-f29a-4448-be62-51cf8ae7defd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T15:37:37.244268300Z",
     "start_time": "2023-07-27T15:36:26.383047900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = torchvision.transforms.Compose([#transforms.CenterCrop(size=300),\n",
    "                                                    #transforms.RandomRotation(degrees=180),\n",
    "                                                    transforms.Grayscale(), \n",
    "                                                    #transforms.TenCrop(size=crop_size),\n",
    "                                                    #transforms.RandomRotation(degrees=[0,90,180,270]),\n",
    "                                                    #transforms.RandomVerticalFlip(p=0.5), #Commented out\n",
    "                                                    #transforms.RandomHorizontalFlip(p=0.5), #Commented out\n",
    "                                                    transforms.FiveCrop(size=crop_size), #changed back to fivecrop\n",
    "                                                    #transforms.functional.vflip(),\n",
    "                                                    #transforms.functional.hflip(),\n",
    "                                                    transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),\n",
    "                                                    transforms.ConvertImageDtype(torch.float32)])\n",
    "                                                    #transforms.PILToTensor()])\n",
    "\n",
    "#use training set for now\n",
    "dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='train', partition=1, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "\n",
    "sampler = data.RandomSampler(dtd_dataset)\n",
    "\n",
    "dtd_dataloader = DataLoader(dtd_dataset, \n",
    "                            sampler=sampler,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d800e7a-2310-48a7-a262-d9360b1e7f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solver: pyramid=UBBBBL_6 pooling=WholeImagePooling()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import utils.brucenet as bn\n",
    "brucy = bn.BruceNet(pooling_region_size=1e20, pyramid_params=False, dummy_img = torch.zeros(20,\n",
    "                                     1,\n",
    "                                     128,\n",
    "                                     128)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221c0e7d-307f-418b-8e22-39fa7bee80c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256, 256])\n",
      "torch.Size([2, 1, 256, 256])\n",
      "tensor([[[[0.6902, 0.6078, 0.4549,  ..., 0.2471, 0.5686, 0.4941],\n",
      "          [0.5216, 0.6039, 0.6510,  ..., 0.1373, 0.3412, 0.4471],\n",
      "          [0.5961, 0.5647, 0.5373,  ..., 0.4235, 0.3529, 0.2078],\n",
      "          ...,\n",
      "          [0.8157, 0.8235, 0.7922,  ..., 0.8118, 0.2549, 0.6902],\n",
      "          [0.7882, 0.7882, 0.8039,  ..., 0.5569, 0.3059, 0.6588],\n",
      "          [0.7451, 0.7373, 0.8078,  ..., 0.0980, 0.1333, 0.4039]]],\n",
      "\n",
      "\n",
      "        [[[0.6902, 0.6078, 0.4549,  ..., 0.2471, 0.5686, 0.4941],\n",
      "          [0.5216, 0.6039, 0.6510,  ..., 0.1373, 0.3412, 0.4471],\n",
      "          [0.5961, 0.5647, 0.5373,  ..., 0.4235, 0.3529, 0.2078],\n",
      "          ...,\n",
      "          [0.8157, 0.8235, 0.7922,  ..., 0.8118, 0.2549, 0.6902],\n",
      "          [0.7882, 0.7882, 0.8039,  ..., 0.5569, 0.3059, 0.6588],\n",
      "          [0.7451, 0.7373, 0.8078,  ..., 0.0980, 0.1333, 0.4039]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the image\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/')\n",
    "\n",
    "#### IMAGE INPUT\n",
    "image = cv2.imread('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/cracked_0004.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "x = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "x = x/255.0\n",
    "x_expanded = np.expand_dims(x, axis=0)\n",
    "y = np.stack([x_expanded,x_expanded])\n",
    "y_t = torch.from_numpy(y).to(dtype=torch.float32, device=device)\n",
    "print(y_t.shape)\n",
    "\n",
    "#### IMAGE OUTPUT\n",
    "image2 = cv2.imread('/home/gridsan/ckoevesdi/video_pyramids/Imagestotest/cracked_0004_output.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "x2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "x2 = x2/255.0\n",
    "x2_expanded = np.expand_dims(x2, axis=0)\n",
    "y2 = np.stack([x2_expanded,x2_expanded])\n",
    "y2_t = torch.from_numpy(y2).to(dtype=torch.float32, device=device)\n",
    "\n",
    "print(y2_t.shape)\n",
    "\n",
    "z = brucy(y_t)\n",
    "print(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3437b6a4-9a74-4958-bc5d-878781cfa043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.3124e+00,  7.2757e-01,  3.1034e-02,  1.0191e+00,  7.0558e-01,\n",
      "          5.0501e-01,  1.4072e+00,  9.5747e-01,  6.7666e-01,  1.9167e+00,\n",
      "          1.2684e+00,  8.7650e-01,  2.5521e+00,  1.6056e+00,  1.0568e+00,\n",
      "          3.3451e+00,  1.9729e+00,  1.2068e+00,  1.5141e+00,  5.3149e-01,\n",
      "          3.5780e-01,  8.2767e-02,  1.3187e+00,  3.8904e-01,  2.5544e-01,\n",
      "          4.4462e-02,  1.2095e+00,  3.4537e-01,  2.2622e-01,  6.2879e-02,\n",
      "          1.3607e+00,  4.1452e-01,  2.7599e-01,  4.7838e-02,  1.9015e-01,\n",
      "          1.4798e-01,  1.9556e-01,  1.4629e-01,  1.5052e-01,  1.5561e-01,\n",
      "          2.6892e+00,  1.0968e+00,  7.3752e-01,  2.5578e-01,  2.2895e+00,\n",
      "          7.7289e-01,  5.1918e-01,  2.1007e-01,  2.0768e+00,  6.6318e-01,\n",
      "          4.4528e-01,  1.7380e-01,  2.3863e+00,  8.4024e-01,  5.4612e-01,\n",
      "          2.3020e-01,  3.8560e-01,  2.9841e-01,  3.9966e-01,  2.9081e-01,\n",
      "          3.0297e-01,  3.0434e-01,  4.7825e+00,  2.2492e+00,  1.2736e+00,\n",
      "          4.9241e-01,  4.0711e+00,  1.5979e+00,  9.0197e-01,  3.2144e-01,\n",
      "          3.5929e+00,  1.3059e+00,  7.6976e-01,  2.9107e-01,  4.1085e+00,\n",
      "          1.6673e+00,  9.3500e-01,  3.4861e-01,  8.0609e-01,  6.3193e-01,\n",
      "          8.1231e-01,  5.9329e-01,  6.2915e-01,  6.1166e-01,  6.6976e+00,\n",
      "          3.0668e+00,  3.3755e-01,  5.8857e+00,  2.2451e+00,  2.4939e-01,\n",
      "          5.5689e+00,  2.1351e+00,  2.1076e-01,  5.8856e+00,  2.3165e+00,\n",
      "          2.9251e-01,  1.1030e+00,  9.0233e-01,  1.1131e+00,  8.9019e-01,\n",
      "          8.7266e-01,  9.1346e-01,  1.7139e-01, -1.5438e-03, -1.7412e-01,\n",
      "          1.1887e-01, -1.2244e-02,  1.3026e-01,  7.1980e-04, -3.3427e-02,\n",
      "         -5.2267e-03, -3.9850e-02,  5.3362e-03, -2.2895e-02,  1.1728e-02,\n",
      "         -3.7022e-02,  3.4416e-01, -4.1606e-03, -3.6496e-01,  2.3427e-01,\n",
      "         -2.8465e-02,  2.4996e-01,  1.2362e-02, -1.0909e-01,  4.6749e-04,\n",
      "         -6.6334e-02,  2.1330e-03, -4.6825e-02,  4.1793e-03, -9.3438e-02,\n",
      "          7.1205e-01,  6.6726e-04, -7.3697e-01,  4.6682e-01, -6.3023e-02,\n",
      "          5.0163e-01,  2.7860e-02,  2.5114e-02,  9.3731e-02, -1.5123e-01,\n",
      "         -2.1507e-02, -7.9611e-02,  1.4792e-01, -1.5112e-01,  9.7035e-01,\n",
      "          4.4940e-03, -1.0395e+00,  7.4731e-01, -6.3743e-02,  7.5192e-01],\n",
      "        [ 3.3124e+00,  7.2757e-01,  3.1034e-02,  1.0191e+00,  7.0558e-01,\n",
      "          5.0501e-01,  1.4072e+00,  9.5747e-01,  6.7666e-01,  1.9167e+00,\n",
      "          1.2684e+00,  8.7650e-01,  2.5521e+00,  1.6056e+00,  1.0568e+00,\n",
      "          3.3451e+00,  1.9729e+00,  1.2068e+00,  1.5141e+00,  5.3149e-01,\n",
      "          3.5780e-01,  8.2767e-02,  1.3187e+00,  3.8904e-01,  2.5544e-01,\n",
      "          4.4462e-02,  1.2095e+00,  3.4537e-01,  2.2622e-01,  6.2879e-02,\n",
      "          1.3607e+00,  4.1452e-01,  2.7599e-01,  4.7838e-02,  1.9015e-01,\n",
      "          1.4798e-01,  1.9556e-01,  1.4629e-01,  1.5052e-01,  1.5561e-01,\n",
      "          2.6892e+00,  1.0968e+00,  7.3752e-01,  2.5578e-01,  2.2895e+00,\n",
      "          7.7289e-01,  5.1918e-01,  2.1007e-01,  2.0768e+00,  6.6318e-01,\n",
      "          4.4528e-01,  1.7380e-01,  2.3863e+00,  8.4024e-01,  5.4612e-01,\n",
      "          2.3020e-01,  3.8560e-01,  2.9841e-01,  3.9966e-01,  2.9081e-01,\n",
      "          3.0297e-01,  3.0434e-01,  4.7825e+00,  2.2492e+00,  1.2736e+00,\n",
      "          4.9241e-01,  4.0711e+00,  1.5979e+00,  9.0197e-01,  3.2144e-01,\n",
      "          3.5929e+00,  1.3059e+00,  7.6976e-01,  2.9107e-01,  4.1085e+00,\n",
      "          1.6673e+00,  9.3500e-01,  3.4861e-01,  8.0609e-01,  6.3193e-01,\n",
      "          8.1231e-01,  5.9329e-01,  6.2915e-01,  6.1166e-01,  6.6976e+00,\n",
      "          3.0668e+00,  3.3755e-01,  5.8857e+00,  2.2451e+00,  2.4939e-01,\n",
      "          5.5689e+00,  2.1351e+00,  2.1076e-01,  5.8856e+00,  2.3165e+00,\n",
      "          2.9251e-01,  1.1030e+00,  9.0233e-01,  1.1131e+00,  8.9019e-01,\n",
      "          8.7266e-01,  9.1346e-01,  1.7139e-01, -1.5438e-03, -1.7412e-01,\n",
      "          1.1887e-01, -1.2244e-02,  1.3026e-01,  7.1980e-04, -3.3427e-02,\n",
      "         -5.2267e-03, -3.9850e-02,  5.3362e-03, -2.2895e-02,  1.1728e-02,\n",
      "         -3.7022e-02,  3.4416e-01, -4.1606e-03, -3.6496e-01,  2.3427e-01,\n",
      "         -2.8465e-02,  2.4996e-01,  1.2362e-02, -1.0909e-01,  4.6749e-04,\n",
      "         -6.6334e-02,  2.1330e-03, -4.6825e-02,  4.1793e-03, -9.3438e-02,\n",
      "          7.1205e-01,  6.6726e-04, -7.3697e-01,  4.6682e-01, -6.3023e-02,\n",
      "          5.0163e-01,  2.7860e-02,  2.5114e-02,  9.3731e-02, -1.5123e-01,\n",
      "         -2.1507e-02, -7.9611e-02,  1.4792e-01, -1.5112e-01,  9.7035e-01,\n",
      "          4.4940e-03, -1.0395e+00,  7.4731e-01, -6.3743e-02,  7.5192e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z1 = brucy(y_t)\n",
    "z2 = brucy(y2_t)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48501d09-5210-48b9-9fe0-ecb2d305d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 1, 128, 128])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([20, 5, 1, 128, 128])\n",
      "torch.Size([5, 150])\n",
      "torch.Size([5, 150])\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    for j, (texture_batch, labels) in enumerate(dtd_dataloader):\n",
    "        print(texture_batch.shape)\n",
    "        output = texture_batch[0].to(device)\n",
    "        x = brucy(output)\n",
    "        print(x.shape)\n",
    "        #print(output.shape)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        #print(output[j].squeeze().shape)\n",
    "        #print(output.shape)\n",
    "        expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "        #print(expanded_labels)\n",
    "        print(x.shape)\n",
    "        #fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
    "        #for j, ax in enumerate(axes.flat):\n",
    "        #    ax.imshow(output[j].squeeze().cpu(), cmap='gray')  \n",
    "        #    ax.set_title(f\"Label: {expanded_labels[j]}\")\n",
    "        #    ax.axis('off')\n",
    "        #plt.show()\n",
    "\n",
    "        if j >= 1: \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9bc1ace-ea71-4b53-a99c-799812b607a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150])\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "combined = torch.stack([m, m2, l])\n",
    "min_val = torch.min(combined, dim=0).values\n",
    "max_val = torch.max(combined, dim=0).values\n",
    "scaled_arrays = (combined - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "668e398d-3fba-4531-a115-d449b5b2ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0785880088806152\n",
      "12.657905578613281\n",
      "11.99793815612793\n"
     ]
    }
   ],
   "source": [
    "distance12 = torch.norm(m-m2).item()\n",
    "distance13 = torch.norm(m-l).item()\n",
    "distance23 = torch.norm(m2-l).item()\n",
    "print(distance12)\n",
    "print(distance13)\n",
    "print(distance23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2410e-0cc5-4a02-baaa-1e7df8874cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0269e+00,  2.5052e-01,  2.5835e-03,  3.5287e-01,  1.4133e-01,\n",
      "         6.0690e-02,  4.8922e-01,  1.9187e-01,  8.0292e-02,  6.6268e-01,\n",
      "         2.4793e-01,  9.7723e-02,  8.7824e-01,  3.0875e-01,  1.1192e-01,\n",
      "         1.1461e+00,  3.8954e-01,  1.3539e-01,  9.1143e-01,  1.7055e-01,\n",
      "         1.5570e-01,  1.1884e-02,  7.8519e-01,  1.2556e-01,  9.9646e-02,\n",
      "         7.0813e-03,  4.3344e-01,  5.5126e-02,  4.2770e-02,  5.2652e-03,\n",
      "         4.7866e-01,  4.7112e-02,  3.9045e-02,  4.4383e-03,  6.1167e-02,\n",
      "         2.7428e-02,  3.7792e-02,  2.9758e-02,  2.8323e-02,  1.8578e-02,\n",
      "         2.1071e+00,  5.9343e-01,  3.9090e-01,  8.4151e-02,  1.5138e+00,\n",
      "         3.2417e-01,  2.0095e-01,  6.5048e-02,  8.4057e-01,  1.2562e-01,\n",
      "         9.5079e-02,  1.8136e-02,  1.0036e+00,  1.4242e-01,  9.8856e-02,\n",
      "         2.6302e-02,  1.8749e-01,  8.7010e-02,  1.2671e-01,  7.4897e-02,\n",
      "         7.9167e-02,  4.9046e-02,  3.4517e+00,  1.0343e+00,  5.2407e-01,\n",
      "         1.1573e-01,  2.4057e+00,  5.2883e-01,  2.1696e-01,  9.0851e-02,\n",
      "         1.4989e+00,  2.6886e-01,  2.1119e-01,  2.8433e-02,  1.7181e+00,\n",
      "         2.7760e-01,  1.5095e-01,  3.8309e-02,  3.2832e-01,  1.7410e-01,\n",
      "         2.2833e-01,  1.4141e-01,  1.4473e-01,  1.0242e-01,  3.8863e+00,\n",
      "         1.0512e+00,  0.0000e+00,  2.4504e+00,  3.9380e-01,  1.3610e-02,\n",
      "         2.7567e+00,  5.7634e-01,  0.0000e+00,  2.2654e+00,  3.2790e-01,\n",
      "         1.1343e-02,  2.8357e-01,  2.3084e-01,  2.5563e-01,  1.8176e-01,\n",
      "         1.5401e-01,  1.7396e-01,  6.6113e-02,  4.8158e-03, -3.5332e-02,\n",
      "         2.5623e-02, -7.3855e-03,  1.4106e-02, -8.6116e-03,  2.8883e-02,\n",
      "        -3.7204e-03,  3.3917e-04, -5.1518e-04,  1.1446e-02, -8.6035e-04,\n",
      "         5.2734e-03,  2.0527e-01,  1.0511e-02, -1.2710e-01,  5.7272e-02,\n",
      "        -3.2184e-02,  3.3572e-02, -1.4997e-02,  9.4550e-02, -1.8244e-02,\n",
      "         4.7246e-02, -4.5389e-03,  2.1282e-02, -9.8763e-04,  8.1190e-03,\n",
      "         3.5858e-01,  1.3473e-02, -2.2658e-01,  9.6353e-02, -5.1180e-02,\n",
      "         7.6323e-02,  3.8526e-03,  9.6817e-02, -1.9202e-02,  6.3458e-02,\n",
      "         2.7769e-03,  6.1600e-02, -3.3875e-03,  1.3376e-02,  3.0272e-01,\n",
      "         4.2381e-03, -2.6522e-01,  1.4864e-01, -3.9029e-02,  1.5603e-01],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m = x[1,:]\n",
    "print(m)\n",
    "m2 = x[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024717f2-f30e-49be-89e4-c81ac166e0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "734afc68-1d85-49f4-bf20-61135f42fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = m\n",
    "l2 = m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de6860-43e3-49ca-bbf5-00c492f2d669",
   "metadata": {},
   "source": [
    "## Test Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f1091-0831-4a35-b4c2-a9c593fd16e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89478886-9627-479f-8c88-4ad8da1b253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextureClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(150, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934b2925-1429-4bb8-918f-a0df9c10da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solver: pyramid=UBBBBL_6 pooling=WholeImagePooling()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022a/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = TextureClassifier(47)  # replace `number_of_classes` with your actual number\n",
    "model = model.to(device)\n",
    "statnet_model = sne.StatNetEncoder(img_size=(crop_size,crop_size),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_stats=num_stats,\n",
    "                                   device=device)\n",
    "statnet_model.to(device)\n",
    "stat_labels = statnet_model.getsstatlabels(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b58f87-dacf-4772-a3cc-86f89212178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second try\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "# Create data indices for training and validation splits\n",
    "dataset_size = len(dtd_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dtd_dataset, batch_size=batch_size, sampler=train_sampler, shuffle = False)\n",
    "validation_loader = DataLoader(dtd_dataset, batch_size=batch_size, sampler=valid_sampler, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e50eb-5456-4c5e-8e44-02b29d7fe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in train_loader:\n",
    "    print(x.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffd36-2761-4226-ab51-a9206829074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, (texture_batch, labels) in enumerate(train_loader):\n",
    "        # Move data to device and flatten it\n",
    "        output = texture_batch.to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        \n",
    "        # Expand labels to account for crops\n",
    "        expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "        \n",
    "        # Skip if batch is smaller than usual\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        if current_batch_size < batch_size:\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        stats = statnet_model.encoder(output)\n",
    "        print(stats[1,:])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(stats)\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs, expanded_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += expanded_labels.size(0)\n",
    "        correct += (predicted == expanded_labels).sum().item()\n",
    "\n",
    "    # Print statistics for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Accuracy: {100 * correct / total}\")\n",
    "\n",
    "\n",
    "    # Initialize variables to keep track of correct predictions and total predictions\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Disable gradient computation to speed up validation\n",
    "    with torch.no_grad():\n",
    "        for j, (texture_batch, labels) in enumerate(validation_loader):\n",
    "            # Prepare data\n",
    "            output = texture_batch.to(device)\n",
    "            output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "            expanded_labels = torch.repeat_interleave(labels, repeats=5, dim=0).to(device)\n",
    "\n",
    "            # Skip if the batch is not full-sized\n",
    "            current_batch_size = output.shape[0] // num_crops\n",
    "            if current_batch_size < batch_size:\n",
    "                continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "            # Forward pass through the models\n",
    "            stats = statnet_model.encoder(output)\n",
    "            outputs = model(stats)\n",
    "\n",
    "            # Calculate predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Update counters\n",
    "            total_correct += (predicted == expanded_labels).sum().item()\n",
    "            total_predictions += expanded_labels.size(0)\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    accuracy = 100 * total_correct / total_predictions\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "    statnet_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cac1f-098b-4e67-be13-818c250fb3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ab3f9-94e8-41f3-89b8-62df2ba4a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop\n",
    "    for j, (texture_batch, labels) in enumerate(train_loader):\n",
    "        output = texture_batch[0].to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        #apply random permutation\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        #if current_batch_size < batch_size:\n",
    "            #continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "\n",
    "        # Assuming stats is a PyTorch tensor of shape (batch_size, 150)\n",
    "        #print(output.shape)\n",
    "        stats = statnet_model.encoder(output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(stats.shape)\n",
    "        outputs = model(stats)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    with torch.no_grad():\n",
    "           for texture_batch, labels in validation_loader:\n",
    "            output = texture_batch.to(device)\n",
    "            labels = torch.tensor([label for label in labels for _ in range(num_crops)]).to(device)\n",
    "            current_batch_size = output.shape[0] // num_crops\n",
    "\n",
    "            #if current_batch_size < batch_size:\n",
    "                #continue  # Skip this batch if it's smaller than the usual batch size\n",
    "            \n",
    "            stats = statnet_model.encoder(output)\n",
    "            outputs = model(stats)\n",
    "            loss = criterion(outputs, texture_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += texture_labels.size(0)\n",
    "            print(total)\n",
    "            correct += (predicted == texture_labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551e756-4a61-4650-b561-701e6d7d8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    for j, texture_batch in enumerate(dtd_dataloader):\n",
    "        #grab texture batch and generate matching labels\n",
    "        output = texture_batch[0].to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        \n",
    "        if current_batch_size < batch_size:\n",
    "            continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "\n",
    "        #calculate stats\n",
    "        stats_vector = statnet_model.forward(output) #statsvector torch.Size([batchsize*crops, 50])\n",
    "        #print(texture_labels.shape)\n",
    "        #print(stats_vector.shape)\n",
    "        \n",
    "        #loss definitions\n",
    "        representation_loss = loss_func_contrastive(stats_vector, texture_labels)\n",
    "        sparsity_loss = statnet_model.sparse_loss()\n",
    "        loss = (representation_loss + \n",
    "                (sparsity_penalty * sparsity_loss))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "\n",
    "        print('*',end='')\n",
    "        #training_loss.append(loss.item())      \n",
    "        if(j%30==0):\n",
    "            print(loss.item())\n",
    "        if(j==100):\n",
    "            break;\n",
    "        training_representation_loss.append(representation_loss.item())\n",
    "        training_sparsity_loss.append(loss.item())    \n",
    "        training_loss.append(loss.item())\n",
    "    num_total_epochs = num_total_epochs + 1\n",
    "    print(f'Finished Epoch {i}. Loss at {loss}.')\n",
    "    #print('Initial Weights:', statnet_model._w)    \n",
    "    if(i%10==0):\n",
    "        compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "        plt.imshow(compressor_mat)\n",
    "        plt.colorbar() \n",
    "        plt.show()\n",
    "    torch.save(\n",
    "            {'state_dict': statnet_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_loss':training_loss,\n",
    "            'last_output':output,\n",
    "            'last_statvec':stats_vector},\n",
    "            f'./model_checkpoint_epoch_{i}.pth')\n",
    "print('All Done!')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047022-f77a-4eb7-955c-3889f33335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef973f0-65ab-44a2-b81f-eca0581488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(compressor_mat)\n",
    "plt.colorbar()\n",
    "print(compressor_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805de811-fdbf-48ab-baeb-517e15242ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_sums = np.sum(np.abs(compressor_mat),axis=0) #how weighted is each of the 150 stats?\n",
    "stat_index_array = np.argsort(stat_sums)[::-1] #get their importance order\n",
    "\n",
    "ordered_stat_sums = stat_sums[stat_index_array]\n",
    "ordered_stat_labels = np.array(stat_labels,dtype=object)[stat_index_array]\n",
    "ordered_stats_labels_pretty = [' '.join([''.join(str(item)) for item in row if item not in ['',None]]) for row in ordered_stat_labels]\n",
    "\n",
    "n=25\n",
    "print(f'Most Important {n} Stats:')\n",
    "for i in range(n):\n",
    "    print(ordered_stat_sums[i],ordered_stats_labels_pretty[i])\n",
    "\n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[:n]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "top_stats = ordered_stats_labels_pretty[:n]\n",
    "ax.set_yticks(np.arange(len(top_stats)), labels=top_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    \n",
    "    \n",
    "#plot most important stats for contrastive learning\n",
    "#plt.imshow(compressor_mat[:,stat_index_array[:n]].T)\n",
    "\n",
    "print(f'Least Important {n} Stats:')\n",
    "for i in range(1,n+1):\n",
    "    print(ordered_stat_sums[-i],ordered_stats_labels_pretty[-i])\n",
    "    \n",
    "    \n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[-n:]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "bot_stats = ordered_stats_labels_pretty[-n:][::-1]\n",
    "ax.set_yticks(np.arange(len(bot_stats)), labels=bot_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317dc4c-4cbd-4429-b13d-deb0e2ea987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?list.insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62797e26-2dbc-4230-9f05-91c2d009bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.8/site-packages/')\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "#texture_labels\n",
    "#output\n",
    "#calculate stats\n",
    "stats_vector = statnet_model.forward(output)\n",
    "print(stats_vector.shape)\n",
    "\n",
    "X = stats_vector  # shape (n_samples, d)\n",
    "X_emb = TSNE(n_components=2, perplexity=10, n_iter=10000, verbose=True).fit_transform(X)  # returns shape (n_samples, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d92a0e-05c9-43de-b300-6081a03d1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "#texture_labels\n",
    "#output\n",
    "#calculate stats\n",
    "stats_vector = statnet_model.forward(output)\n",
    "print(stats_vector.shape)\n",
    "\n",
    "X = stats_vector  # shape (n_samples, d)\n",
    "X_emb = TSNE(n_components=2, perplexity=10, n_iter=10000, verbose=True).fit_transform(X)  # returns shape (n_samples, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19995b08-a1c6-43d6-b9c1-365be30a9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plt.legend(handles=scatter.legend_elements()[0], labels=texture_labels, title=\"Textures\")\n",
    "texture_label_list = texture_labels.numpy()\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#scatter = ax.scatter(X_emb[:,0],X_emb[:,1],c=texture_label_list,label=texture_label_list)\n",
    "#plt.legend()#handles=scatter.legend_elements()[0], labels=texture_label_list)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.legend(handles=scatter.legend_elements()[0],title=\"texture\")\n",
    "\n",
    "#handles = [plt.plot([],color=sc.get_cmap()(sc.norm(c)),ls=\"\", marker=\"o\")[0] for c,l in clset ]\n",
    "#labels = [l for c,l in clset]\n",
    "#ax.legend(texture_label_list)\n",
    "\n",
    "unique_textures = set(texture_label_list)\n",
    "\n",
    "#fix, ax = plt.subplots()\n",
    "plt.figure(figsize=(6,6))\n",
    "for i, texture in enumerate(unique_textures):\n",
    "    texture_index = np.where(texture_label_list == texture)[0]\n",
    "    texture_index = np.where(texture_label_list==i)[0]\n",
    "    plt.scatter(X_emb[texture_index,0],X_emb[texture_index,1],label=i)\n",
    "plt.legend()\n",
    "plt.title(f'T-sne embeddings: {num_crops} crops, {batch_size} textures')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, label in enumerate(texture_label_list):\n",
    "    plt.annotate(label, (X_emb[i,0],X_emb[i,1] + 0.2))\n",
    "\n",
    "#for i, txt in enumerate(texture_label_list):\n",
    "#    plt.annotate(texture_label_list, (X_emb[:,0],X_emb[:,1]))\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e10161-e890-4977-af82-c0cae58a4105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
