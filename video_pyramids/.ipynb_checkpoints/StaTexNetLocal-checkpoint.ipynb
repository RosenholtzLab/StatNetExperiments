{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c9fe52-26af-4089-803e-4406a29432f3",
   "metadata": {},
   "source": [
    "# StaTexNet - Network Encoding Statistics for Textures\n",
    "Locally Enforced Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc538b7-27ab-46c2-be44-33847dda73a7",
   "metadata": {},
   "source": [
    "## Dependencies & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb413b8-72ca-4a7a-b98f-d32cc73b5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "import utils.statnetencoder as sne\n",
    "import imp\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append('../')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#hyperparams\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "crop_size = 128\n",
    "num_stats = 200\n",
    "#optimizer_type='adam'\n",
    "optimizer_type='sgd'\n",
    "learning_rate = 0.01\n",
    "num_crops = 5\n",
    "\n",
    "#dataset location\n",
    "dtd_folder = '~/data/dtd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8f4b6b-4974-469e-bb01-8661a482c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 10])\n",
      "torch.Size([1, 2, 10, 10])\n",
      "torch.Size([1, 2, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(2, 10, 10)\n",
    "upsample = nn.Upsample(size=24, mode='bilinear')\n",
    "print(A.shape)\n",
    "A = torch.unsqueeze(A, 0)\n",
    "print(A.shape)\n",
    "A = upsample(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806091f-63d7-4819-ad4e-2ef117a7097d",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1482b4a6-f29a-4448-be62-51cf8ae7defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_transforms = torchvision.transforms.Compose([#transforms.CenterCrop(size=300),\n",
    "                                                    #transforms.RandomRotation(degrees=180),\n",
    "                                                    transforms.Grayscale(),\n",
    "                                                    #transforms.TenCrop(size=crop_size),\n",
    "                                                    #transforms.RandomRotation(degrees=[0,90,180,270]),\n",
    "                                                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                                                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                    transforms.FiveCrop(size=crop_size),\n",
    "                                                    #transforms.functional.vflip(),\n",
    "                                                    #transforms.functional.hflip(),\n",
    "                                                    transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),\n",
    "                                                    transforms.ConvertImageDtype(torch.float32)])\n",
    "                                                    #transforms.PILToTensor()])\n",
    "\n",
    "#use training set for now\n",
    "dtd_dataset = torchvision.datasets.DTD(root='~/data/dtd_torch', split='train', partition=1, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=True)\n",
    "\n",
    "sampler = data.RandomSampler(dtd_dataset)\n",
    "\n",
    "dtd_dataloader = DataLoader(dtd_dataset, \n",
    "                            sampler=sampler,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "#dtd_labels = tf\n",
    "\n",
    "tensor2pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de6860-43e3-49ca-bbf5-00c492f2d669",
   "metadata": {},
   "source": [
    "## Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bad27ed-cc59-4d7f-95c7-5ed5d29c3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, texture_batch in enumerate(dtd_dataloader):\n",
    "#     #grab texture batch and generate matching labels\n",
    "#     output = texture_batch[0].to(device)\n",
    "#     output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "#     texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "#     #apply random permutation\n",
    "#     perm = torch.randperm(batch_size * num_crops)\n",
    "#     output = output[perm]\n",
    "#     texture_labels = texture_labels[perm]\n",
    "#     print(texture_labels)\n",
    "#     #loop through batch and plot images\n",
    "#     for j in range(batch_size):\n",
    "#         plt.figure(figsize=(8,4))\n",
    "#         for i in range(num_crops):\n",
    "#             plt.subplot(2,5,i+1)\n",
    "#             plt.imshow(tensor2pil_transform(output[i+j*num_crops,:,:,:]))\n",
    "#             plt.axis('off')\n",
    "#         plt.show()\n",
    "#     if(n==1):\n",
    "#         break;\n",
    "    \n",
    "#tensor2pil_transform(output[4,0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f1091-0831-4a35-b4c2-a9c593fd16e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103f2a6c-df93-4147-a57a-cd4cb1e7b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height of pyramid is, 5\n"
     ]
    }
   ],
   "source": [
    "imp.reload(sne)\n",
    "statnet_model = sne.StatNetEncoder(img_size=(crop_size,crop_size),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_stats=num_stats,\n",
    "                                   vectorized=False,\n",
    "                                   device=device)\n",
    "statnet_model.to(device)\n",
    "\n",
    "#optimizer\n",
    "if(optimizer_type=='sgd'):\n",
    "    optimizer = torch.optim.SGD(statnet_model.parameters(), lr=learning_rate)#, momentum=learning_momentum)\n",
    "elif(optimizer_type=='adam'):\n",
    "    optimizer = torch.optim.Adam(statnet_model.parameters(), lr=learning_rate)\n",
    "elif(optimizer_type=='adagrad'):\n",
    "    optimizer = torch.optim.Adagrad(statnet_model.parameters(), lr=learning_rate)\n",
    "elif(optimizer_type=='adadelta'):\n",
    "    optimizer = torch.optim.Adadelta(statnet_model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    print('No Optimizer Specified! Adam is default!')\n",
    "    optimizer = torch.optim.Adam(statnet_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cac1f-098b-4e67-be13-818c250fb3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0551e756-4a61-4650-b561-701e6d7d8533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training:\n",
      "height of pyramid is, 5\n",
      "torch.Size([50, 1, 128, 128, 2])\n",
      "torch.Size([50, 1, 128, 128, 2])\n",
      "torch.Size([50, 1, 128, 128, 2])\n",
      "torch.Size([50, 1, 128, 128, 2])\n",
      "torch.Size([50, 1, 64, 64, 2])\n",
      "torch.Size([50, 1, 64, 64, 2])\n",
      "torch.Size([50, 1, 64, 64, 2])\n",
      "torch.Size([50, 1, 64, 64, 2])\n",
      "torch.Size([50, 1, 32, 32, 2])\n",
      "torch.Size([50, 1, 32, 32, 2])\n",
      "torch.Size([50, 1, 32, 32, 2])\n",
      "torch.Size([50, 1, 32, 32, 2])\n",
      "level pyr torch.Size([50, 26, 128, 128])\n",
      "torch.Size([50, 26, 128, 128])\n",
      "torch.Size([50, 200, 121, 121])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "embeddings must be a 2D tensor of shape (batch_size, embedding_size)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#loss definitions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats_vector\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexture_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_metric_learning/losses/base_metric_loss_function.py:34\u001b[0m, in \u001b[0;36mBaseMetricLossFunction.forward\u001b[0;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    embeddings: tensor of size (batch_size, embedding_size)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mReturns: the loss\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_stats()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mc_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mto_device(labels, embeddings)\n\u001b[1;32m     36\u001b[0m ref_emb, ref_labels \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mset_ref_emb(embeddings, labels, ref_emb, ref_labels)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_metric_learning/utils/common_functions.py:415\u001b[0m, in \u001b[0;36mcheck_shapes\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of embeddings must equal number of labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings must be a 2D tensor of shape (batch_size, embedding_size)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels must be a 1D tensor of shape (batch_size,)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: embeddings must be a 2D tensor of shape (batch_size, embedding_size)"
     ]
    }
   ],
   "source": [
    "imp.reload(sne)\n",
    "loss_func = losses.GeneralizedLiftedStructureLoss()\n",
    "\n",
    "training_loss = []\n",
    "statnet_model.train() # Set model to training mode\n",
    "optimizer.zero_grad()\n",
    "statnet_model.zero_grad()\n",
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    for j, texture_batch in enumerate(dtd_dataloader):\n",
    "        #grab texture batch and generate matching labels\n",
    "        output = texture_batch[0].to(device)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        #apply random permutation\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "\n",
    "        #calculate stats\n",
    "        stats_vector = statnet_model.encode(output)\n",
    "        #loss definitions\n",
    "        print(stats_vector.shape)\n",
    "        loss = loss_func(stats_vector, texture_labels)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        print('*',end='')\n",
    "        #training_loss.append(loss.item())      \n",
    "        if(j%30==0):\n",
    "            print(loss.item())\n",
    "        if(j==100):\n",
    "            break;\n",
    "            \n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "    print(f'Finished Epoch {i}. Loss at {loss}.')\n",
    "    # torch.save(\n",
    "    #         {'state_dict': statnet_model.state_dict(),\n",
    "    #         'optimizer_state_dict': optimizer.state_dict()},\n",
    "    #         f'{model_save_folder}/model_checkpoint_epoch_{i}.pth')\n",
    "    \n",
    "print('All Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047022-f77a-4eb7-955c-3889f33335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef973f0-65ab-44a2-b81f-eca0581488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(gpu) for gpu in args.gpus])\n",
    "\n",
    "print(\"Available/CUDA_VISIBLE_DEVICES\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805de811-fdbf-48ab-baeb-517e15242ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n#steerable pyramid\n",
    "\n",
    "im_batch = im_batch.to(device).float()\n",
    "\n",
    "pyr_torch = SCFpyr_PyTorch(pyr_height, pyr_nbands, device=device)\n",
    "coeff_torch = pyr_torch.build(im_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18f54b-72b4-43ed-8847-94eaab37d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317dc4c-4cbd-4429-b13d-deb0e2ea987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?list.insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62797e26-2dbc-4230-9f05-91c2d009bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
