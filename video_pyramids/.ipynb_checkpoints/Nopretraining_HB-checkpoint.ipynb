{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2a99f3-fe17-423e-a5a3-a719855f82c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification without contrastive learning pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5cdd24-e50f-4fdc-8c06-15cb7906f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tsne_torch import TorchTSNE\n",
    "import numpy as np\n",
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.9/site-packages/')\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PooledStatisticsMetamers/poolstatmetamer/')\n",
    "import utils.statnetencoder as sne\n",
    "import importlib\n",
    "import imp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\chris\\Documents\\MIT\\Statistics_analysis_code\\PyTorchSteerablePyramid')\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PyTorchSteerablePyramid/')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "dtd_folder = 'home/gridsan/ckoevesdi/data/OT/dtd_torch/dtd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545b3cb5-7aa1-4709-b07f-35028fa29903",
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(size=300),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # This will also convert the image from [0, 255] to [0.0, 1.0]\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "class_dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='train', \n",
    "                                             partition=10, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "# Define the batch size (Change this based on your requirements)\n",
    "batch_size = 1\n",
    "\n",
    "# Create a DataLoader\n",
    "classification_dataloader = DataLoader(\n",
    "    class_dtd_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "tensor2pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191d24c5-57f2-437e-b39b-a13bbaec5f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solver: pyramid=UBBBBL_6 pooling=WholeImagePooling()\n"
     ]
    }
   ],
   "source": [
    "import utils.brucenet as bn\n",
    "brucy = bn.BruceNet(pooling_region_size=1e20, pyramid_params=False, dummy_img = torch.zeros(20,\n",
    "                                     1,\n",
    "                                     300,\n",
    "                                     300)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59f03dd-9c93-4dd7-ab50-05f4b92e3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_statistics = []\n",
    "\n",
    "for j, (texture_batch, labels) in enumerate(classification_dataloader):\n",
    "    # Assuming texture_batch has a shape of [20, 5, 1, 128, 128]\n",
    "    #print(texture_batch.shape)\n",
    "    output = texture_batch.to(device)\n",
    "    output = torch.stack([output, output]).squeeze(1)\n",
    "    #print(output.shape)\n",
    "    #print(labels)\n",
    "    statistics = brucy(output)  # This should output a tensor of shape [2, 150]\n",
    "    statistics = statistics[0,:]\n",
    "    #print(statistics[0,:])\n",
    "    # You can now store these statistics, along with the label and original image index\n",
    "    all_statistics.append({\n",
    "        'statistics': statistics.cpu().numpy(),\n",
    "        #'label': labels[i].item(),\n",
    "        'original_image_index': labels.cpu().numpy()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61aed4b7-a1ac-4127-9418-4fb13a30bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4185, 150)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Convert the list of dictionaries to a NumPy array or PyTorch tensor\n",
    "statistics_array = np.array([item['statistics'] for item in all_statistics])\n",
    "labels_array = np.array([item['original_image_index'] for item in all_statistics])\n",
    "print(statistics_array.shape)\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(statistics_array)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "statistics_tensor = torch.tensor(scaled_array, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "#print(statistics_tensor[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c5d45b3f-0674-4507-93a4-29576892f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatisticsDataset(Dataset):\n",
    "    def __init__(self, statistics_tensor, labels_tensor):\n",
    "        self.statistics = statistics_tensor\n",
    "        self.labels = labels_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.statistics[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset object\n",
    "statistics_dataset = StatisticsDataset(statistics_tensor, labels_tensor)\n",
    "\n",
    "batches = 100\n",
    "class_statistics_dataloader = DataLoader(statistics_dataset, batch_size=batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8e9b8571-6b3e-4ef9-9ffd-97ba93776e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingModel(\n",
      "  (layer1): Linear(in_features=150, out_features=100, bias=True)\n",
      "  (layer2): Linear(in_features=100, out_features=75, bias=True)\n",
      "  (layer3): Linear(in_features=75, out_features=50, bias=True)\n",
      "  (layer4): Linear(in_features=50, out_features=47, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(150, 100)\n",
    "        self.layer2 = nn.Linear(100, 75)\n",
    "        self.layer3 = nn.Linear(75, 50)\n",
    "        self.layer4 = nn.Linear(50, 47)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "model = EmbeddingModel()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "25ea9023-aa50-4476-823a-515bd1a55bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(150, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.layer4 = nn.Linear(64, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 47)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout(F.relu(self.layer2(x)))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(F.relu(self.layer4(x)))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        x = self.layer6(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d61bd-9545-4d93-9ff4-2c23174c4cad",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "98be3e70-b8d0-4ae9-8d4d-6a88e837c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "#model = EmbeddingModel()\n",
    "model = ImprovedModel()\n",
    "\n",
    "model.to(device)\n",
    "classification_loss = CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "training_loss = []\n",
    "training_representation_loss = []\n",
    "training_sparsity_loss = []\n",
    "num_total_epochs = 0\n",
    "#[s for s in stat_labels] #print all stats with details\n",
    "model.train() # Set model to training mode\n",
    "optimizer.zero_grad()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "aa1eab9b-1b43-4d08-9dbc-6c96755bdbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/170], Loss: 3.8123, Accuracy: 3.03%\n",
      "Epoch [2/170], Loss: 3.7825, Accuracy: 4.28%\n",
      "Epoch [3/170], Loss: 3.8054, Accuracy: 5.40%\n",
      "Epoch [4/170], Loss: 3.6768, Accuracy: 6.55%\n",
      "Epoch [5/170], Loss: 3.5867, Accuracy: 7.93%\n",
      "Epoch [6/170], Loss: 3.5284, Accuracy: 10.25%\n",
      "Epoch [7/170], Loss: 3.3839, Accuracy: 10.49%\n",
      "Epoch [8/170], Loss: 3.3847, Accuracy: 12.69%\n",
      "Epoch [9/170], Loss: 3.1668, Accuracy: 13.98%\n",
      "Epoch [10/170], Loss: 3.2154, Accuracy: 14.48%\n",
      "Epoch [11/170], Loss: 3.1408, Accuracy: 15.05%\n",
      "Epoch [12/170], Loss: 3.4577, Accuracy: 15.17%\n",
      "Epoch [13/170], Loss: 3.2165, Accuracy: 16.89%\n",
      "Epoch [14/170], Loss: 3.2978, Accuracy: 17.40%\n",
      "Epoch [15/170], Loss: 3.2372, Accuracy: 17.95%\n",
      "Epoch [16/170], Loss: 3.1110, Accuracy: 19.02%\n",
      "Epoch [17/170], Loss: 2.9301, Accuracy: 19.62%\n",
      "Epoch [18/170], Loss: 2.9814, Accuracy: 19.98%\n",
      "Epoch [19/170], Loss: 2.8196, Accuracy: 21.19%\n",
      "Epoch [20/170], Loss: 2.9868, Accuracy: 21.05%\n",
      "Epoch [21/170], Loss: 2.8437, Accuracy: 21.74%\n",
      "Epoch [22/170], Loss: 2.7693, Accuracy: 22.13%\n",
      "Epoch [23/170], Loss: 2.7165, Accuracy: 21.74%\n",
      "Epoch [24/170], Loss: 2.8826, Accuracy: 21.77%\n",
      "Epoch [25/170], Loss: 2.7719, Accuracy: 22.96%\n",
      "Epoch [26/170], Loss: 2.7649, Accuracy: 23.23%\n",
      "Epoch [27/170], Loss: 2.8821, Accuracy: 23.46%\n",
      "Epoch [28/170], Loss: 2.7650, Accuracy: 24.73%\n",
      "Epoch [29/170], Loss: 2.7401, Accuracy: 25.14%\n",
      "Epoch [30/170], Loss: 2.8484, Accuracy: 24.97%\n",
      "Epoch [31/170], Loss: 2.8739, Accuracy: 23.66%\n",
      "Epoch [32/170], Loss: 2.6894, Accuracy: 24.30%\n",
      "Epoch [33/170], Loss: 2.4778, Accuracy: 25.19%\n",
      "Epoch [34/170], Loss: 2.7137, Accuracy: 26.36%\n",
      "Epoch [35/170], Loss: 2.6104, Accuracy: 24.64%\n",
      "Epoch [36/170], Loss: 2.6048, Accuracy: 26.21%\n",
      "Epoch [37/170], Loss: 2.6702, Accuracy: 25.95%\n",
      "Epoch [38/170], Loss: 2.6352, Accuracy: 26.09%\n",
      "Epoch [39/170], Loss: 2.6940, Accuracy: 27.31%\n",
      "Epoch [40/170], Loss: 2.5629, Accuracy: 27.65%\n",
      "Epoch [41/170], Loss: 2.6812, Accuracy: 27.41%\n",
      "Epoch [42/170], Loss: 2.6061, Accuracy: 27.67%\n",
      "Epoch [43/170], Loss: 2.9217, Accuracy: 28.48%\n",
      "Epoch [44/170], Loss: 2.9107, Accuracy: 27.84%\n",
      "Epoch [45/170], Loss: 2.6653, Accuracy: 28.24%\n",
      "Epoch [46/170], Loss: 2.6495, Accuracy: 29.01%\n",
      "Epoch [47/170], Loss: 2.5044, Accuracy: 29.08%\n",
      "Epoch [48/170], Loss: 2.4250, Accuracy: 27.69%\n",
      "Epoch [49/170], Loss: 2.5100, Accuracy: 31.23%\n",
      "Epoch [50/170], Loss: 2.3180, Accuracy: 29.92%\n",
      "Epoch [51/170], Loss: 2.4360, Accuracy: 28.82%\n",
      "Epoch [52/170], Loss: 2.2995, Accuracy: 29.68%\n",
      "Epoch [53/170], Loss: 2.2819, Accuracy: 30.18%\n",
      "Epoch [54/170], Loss: 2.3865, Accuracy: 30.44%\n",
      "Epoch [55/170], Loss: 2.6181, Accuracy: 29.87%\n",
      "Epoch [56/170], Loss: 2.6621, Accuracy: 30.16%\n",
      "Epoch [57/170], Loss: 2.4961, Accuracy: 30.54%\n",
      "Epoch [58/170], Loss: 2.6384, Accuracy: 30.37%\n",
      "Epoch [59/170], Loss: 2.4616, Accuracy: 30.97%\n",
      "Epoch [60/170], Loss: 2.3723, Accuracy: 31.04%\n",
      "Epoch [61/170], Loss: 2.4364, Accuracy: 31.71%\n",
      "Epoch [62/170], Loss: 2.3897, Accuracy: 31.73%\n",
      "Epoch [63/170], Loss: 2.4801, Accuracy: 32.09%\n",
      "Epoch [64/170], Loss: 2.0435, Accuracy: 33.09%\n",
      "Epoch [65/170], Loss: 2.5081, Accuracy: 32.62%\n",
      "Epoch [66/170], Loss: 2.5840, Accuracy: 32.69%\n",
      "Epoch [67/170], Loss: 2.3524, Accuracy: 32.59%\n",
      "Epoch [68/170], Loss: 2.3842, Accuracy: 32.54%\n",
      "Epoch [69/170], Loss: 2.2920, Accuracy: 33.19%\n",
      "Epoch [70/170], Loss: 2.4932, Accuracy: 33.14%\n",
      "Epoch [71/170], Loss: 2.3264, Accuracy: 32.83%\n",
      "Epoch [72/170], Loss: 2.2854, Accuracy: 33.19%\n",
      "Epoch [73/170], Loss: 2.5018, Accuracy: 33.02%\n",
      "Epoch [74/170], Loss: 2.4975, Accuracy: 32.76%\n",
      "Epoch [75/170], Loss: 2.3265, Accuracy: 34.19%\n",
      "Epoch [76/170], Loss: 2.1865, Accuracy: 33.95%\n",
      "Epoch [77/170], Loss: 2.5204, Accuracy: 33.88%\n",
      "Epoch [78/170], Loss: 2.1845, Accuracy: 34.07%\n",
      "Epoch [79/170], Loss: 2.5116, Accuracy: 34.50%\n",
      "Epoch [80/170], Loss: 2.3888, Accuracy: 33.62%\n",
      "Epoch [81/170], Loss: 2.0888, Accuracy: 34.50%\n",
      "Epoch [82/170], Loss: 2.0520, Accuracy: 35.65%\n",
      "Epoch [83/170], Loss: 2.2140, Accuracy: 34.24%\n",
      "Epoch [84/170], Loss: 2.3941, Accuracy: 34.43%\n",
      "Epoch [85/170], Loss: 2.3983, Accuracy: 35.87%\n",
      "Epoch [86/170], Loss: 2.3393, Accuracy: 34.55%\n",
      "Epoch [87/170], Loss: 2.1665, Accuracy: 35.99%\n",
      "Epoch [88/170], Loss: 2.1957, Accuracy: 36.68%\n",
      "Epoch [89/170], Loss: 2.7361, Accuracy: 34.34%\n",
      "Epoch [90/170], Loss: 2.2331, Accuracy: 35.15%\n",
      "Epoch [91/170], Loss: 1.9027, Accuracy: 36.54%\n",
      "Epoch [92/170], Loss: 2.1832, Accuracy: 36.75%\n",
      "Epoch [93/170], Loss: 2.6582, Accuracy: 35.70%\n",
      "Epoch [94/170], Loss: 2.4667, Accuracy: 35.89%\n",
      "Epoch [95/170], Loss: 2.2929, Accuracy: 35.34%\n",
      "Epoch [96/170], Loss: 2.3899, Accuracy: 36.20%\n",
      "Epoch [97/170], Loss: 2.0442, Accuracy: 36.44%\n",
      "Epoch [98/170], Loss: 2.5283, Accuracy: 35.94%\n",
      "Epoch [99/170], Loss: 2.2303, Accuracy: 36.87%\n",
      "Epoch [100/170], Loss: 2.3385, Accuracy: 37.32%\n",
      "Epoch [101/170], Loss: 2.2590, Accuracy: 37.68%\n",
      "Epoch [102/170], Loss: 1.9534, Accuracy: 36.99%\n",
      "Epoch [103/170], Loss: 2.5225, Accuracy: 37.06%\n",
      "Epoch [104/170], Loss: 2.3744, Accuracy: 37.61%\n",
      "Epoch [105/170], Loss: 2.0637, Accuracy: 37.71%\n",
      "Epoch [106/170], Loss: 2.1985, Accuracy: 38.04%\n",
      "Epoch [107/170], Loss: 1.9857, Accuracy: 38.83%\n",
      "Epoch [108/170], Loss: 2.1234, Accuracy: 38.33%\n",
      "Epoch [109/170], Loss: 2.1172, Accuracy: 38.85%\n",
      "Epoch [110/170], Loss: 1.9563, Accuracy: 37.59%\n",
      "Epoch [111/170], Loss: 2.6355, Accuracy: 38.76%\n",
      "Epoch [112/170], Loss: 1.9177, Accuracy: 37.97%\n",
      "Epoch [113/170], Loss: 2.2736, Accuracy: 39.07%\n",
      "Epoch [114/170], Loss: 2.1447, Accuracy: 37.73%\n",
      "Epoch [115/170], Loss: 1.9116, Accuracy: 38.45%\n",
      "Epoch [116/170], Loss: 1.8489, Accuracy: 39.40%\n",
      "Epoch [117/170], Loss: 2.1375, Accuracy: 38.85%\n",
      "Epoch [118/170], Loss: 2.0497, Accuracy: 40.26%\n",
      "Epoch [119/170], Loss: 2.1346, Accuracy: 39.71%\n",
      "Epoch [120/170], Loss: 2.0442, Accuracy: 40.10%\n",
      "Epoch [121/170], Loss: 2.1199, Accuracy: 39.86%\n",
      "Epoch [122/170], Loss: 2.1363, Accuracy: 38.71%\n",
      "Epoch [123/170], Loss: 1.9827, Accuracy: 39.76%\n",
      "Epoch [124/170], Loss: 2.2182, Accuracy: 39.31%\n",
      "Epoch [125/170], Loss: 2.3159, Accuracy: 39.00%\n",
      "Epoch [126/170], Loss: 2.3282, Accuracy: 39.59%\n",
      "Epoch [127/170], Loss: 1.8416, Accuracy: 39.78%\n",
      "Epoch [128/170], Loss: 2.0888, Accuracy: 40.10%\n",
      "Epoch [129/170], Loss: 2.1256, Accuracy: 40.31%\n",
      "Epoch [130/170], Loss: 2.0150, Accuracy: 40.29%\n",
      "Epoch [131/170], Loss: 2.0026, Accuracy: 40.17%\n",
      "Epoch [132/170], Loss: 2.0672, Accuracy: 40.41%\n",
      "Epoch [133/170], Loss: 1.9259, Accuracy: 40.74%\n",
      "Epoch [134/170], Loss: 2.0457, Accuracy: 41.34%\n",
      "Epoch [135/170], Loss: 2.0515, Accuracy: 40.86%\n",
      "Epoch [136/170], Loss: 1.9448, Accuracy: 40.48%\n",
      "Epoch [137/170], Loss: 1.8765, Accuracy: 41.46%\n",
      "Epoch [138/170], Loss: 2.2283, Accuracy: 40.48%\n",
      "Epoch [139/170], Loss: 1.8721, Accuracy: 41.10%\n",
      "Epoch [140/170], Loss: 2.2424, Accuracy: 40.91%\n",
      "Epoch [141/170], Loss: 2.1493, Accuracy: 41.96%\n",
      "Epoch [142/170], Loss: 2.3012, Accuracy: 41.43%\n",
      "Epoch [143/170], Loss: 1.8749, Accuracy: 40.41%\n",
      "Epoch [144/170], Loss: 2.1595, Accuracy: 41.48%\n",
      "Epoch [145/170], Loss: 1.9467, Accuracy: 42.13%\n",
      "Epoch [146/170], Loss: 1.9369, Accuracy: 42.82%\n",
      "Epoch [147/170], Loss: 2.0357, Accuracy: 41.55%\n",
      "Epoch [148/170], Loss: 2.1557, Accuracy: 40.93%\n",
      "Epoch [149/170], Loss: 1.8415, Accuracy: 42.87%\n",
      "Epoch [150/170], Loss: 2.0717, Accuracy: 42.03%\n",
      "Epoch [151/170], Loss: 2.1054, Accuracy: 40.88%\n",
      "Epoch [152/170], Loss: 2.0938, Accuracy: 42.37%\n",
      "Epoch [153/170], Loss: 2.2288, Accuracy: 41.89%\n",
      "Epoch [154/170], Loss: 2.4029, Accuracy: 42.87%\n",
      "Epoch [155/170], Loss: 2.0662, Accuracy: 41.62%\n",
      "Epoch [156/170], Loss: 1.8181, Accuracy: 41.96%\n",
      "Epoch [157/170], Loss: 2.0213, Accuracy: 43.61%\n",
      "Epoch [158/170], Loss: 1.7868, Accuracy: 43.39%\n",
      "Epoch [159/170], Loss: 2.0730, Accuracy: 43.97%\n",
      "Epoch [160/170], Loss: 2.3531, Accuracy: 43.15%\n",
      "Epoch [161/170], Loss: 1.9077, Accuracy: 44.16%\n",
      "Epoch [162/170], Loss: 2.1949, Accuracy: 43.01%\n",
      "Epoch [163/170], Loss: 2.1481, Accuracy: 43.01%\n",
      "Epoch [164/170], Loss: 2.0674, Accuracy: 43.18%\n",
      "Epoch [165/170], Loss: 2.1347, Accuracy: 43.46%\n",
      "Epoch [166/170], Loss: 2.0217, Accuracy: 43.70%\n",
      "Epoch [167/170], Loss: 1.7461, Accuracy: 43.30%\n",
      "Epoch [168/170], Loss: 1.9180, Accuracy: 44.04%\n",
      "Epoch [169/170], Loss: 2.1412, Accuracy: 44.16%\n",
      "Epoch [170/170], Loss: 1.8063, Accuracy: 43.66%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 170 # Number of epochs for fine-tuning\n",
    "\n",
    "# Initialize variables for tracking accuracy\n",
    "total_samples = 0\n",
    "correct_samples = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in class_statistics_dataloader:  \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.squeeze()\n",
    "        #print(y_batch.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        #print(outputs.shape)\n",
    "        # Compute loss\n",
    "        loss = classification_loss(outputs, y_batch)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += y_batch.size(0)\n",
    "        correct_samples += (predicted == y_batch).sum().item()\n",
    "\n",
    "    # Calculate accuracy for this epoch\n",
    "    epoch_accuracy = 100 * correct_samples / total_samples\n",
    "\n",
    "    # Logging\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    # Reset accuracy tracking variables for the next epoch\n",
    "    total_samples = 0\n",
    "    correct_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e295ca-7175-4f49-80b0-40388052ee7d",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1b18a5e1-62c8-4917-a1c0-3b8d0c629bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(size=300),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # This will also convert the image from [0, 255] to [0.0, 1.0]\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "test_dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='val', \n",
    "                                            partition=10, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "# Define the batch size (Change this based on your requirements)\n",
    "batch_size = 1\n",
    "\n",
    "# Create a DataLoader\n",
    "test_classification_dataloader = DataLoader(\n",
    "    test_dtd_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(test_dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "330e54b5-64f8-4e01-920e-f7d27716c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_statistics = []\n",
    "\n",
    "for j, (texture_batch, labels) in enumerate(test_classification_dataloader):\n",
    "    # Assuming texture_batch has a shape of [20, 5, 1, 128, 128]\n",
    "    #print(texture_batch.shape)\n",
    "    output = texture_batch.to(device)\n",
    "    output = torch.stack([output, output]).squeeze(1)\n",
    "    #print(output.shape)\n",
    "    #print(labels)\n",
    "    statistics = brucy(output)  # This should output a tensor of shape [2, 150]\n",
    "    statistics = statistics[0,:]\n",
    "    #print(statistics[0,:])\n",
    "    # You can now store these statistics, along with the label and original image index\n",
    "    test_all_statistics.append({\n",
    "        'statistics': statistics.cpu().numpy(),\n",
    "        #'label': labels[i].item(),\n",
    "        'original_image_index': labels.cpu().numpy()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0dbd5a3-ebdd-4cd4-806c-4811f6a26c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(939, 150)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Convert the list of dictionaries to a NumPy array or PyTorch tensor\n",
    "test_statistics_array = np.array([item['statistics'] for item in test_all_statistics])\n",
    "test_labels_array = np.array([item['original_image_index'] for item in test_all_statistics])\n",
    "print(test_statistics_array.shape)\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "test_scaled_array = scaler.fit_transform(test_statistics_array)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "test_statistics_tensor = torch.tensor(test_scaled_array, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels_array, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4b655e34-95e6-409b-b408-5302b8630776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatisticsDataset(Dataset):\n",
    "    def __init__(self, statistics_tensor, labels_tensor):\n",
    "        self.statistics = statistics_tensor\n",
    "        self.labels = labels_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.statistics[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset object\n",
    "test_statistics_dataset = StatisticsDataset(test_statistics_tensor, test_labels_tensor)\n",
    "\n",
    "batches = 80\n",
    "test_class_statistics_dataloader = DataLoader(test_statistics_dataset, batch_size=batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "79ec4322-2fb0-4d2f-96c8-961009a5fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 26.62%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImprovedModel(\n",
       "  (layer1): Linear(in_features=150, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (layer5): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (layer6): Linear(in_features=32, out_features=47, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize variables for tracking accuracy\n",
    "total_samples = 0\n",
    "correct_samples = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_class_statistics_dataloader:  \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_batch = y_batch.squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        total_samples += y_batch.size(0)\n",
    "        correct_samples += (predicted == y_batch).sum().item()\n",
    "\n",
    "# Calculate accuracy for the test set\n",
    "test_accuracy = 100 * correct_samples / total_samples\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Set the model back to training mode (optional if you continue training afterwards)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb64244-c40c-4bbd-adb1-119e374c0ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
