{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c9fe52-26af-4089-803e-4406a29432f3",
   "metadata": {},
   "source": [
    "# StaTexNet - Network Encoding Statistics for Textures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc538b7-27ab-46c2-be44-33847dda73a7",
   "metadata": {},
   "source": [
    "## Dependencies & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e95b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:44:59.904549200Z",
     "start_time": "2023-07-27T14:44:59.889457100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('/home/gridsan/ckoevesdi/.local/lib/python3.9/site-packages/')\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PooledStatisticsMetamers/poolstatmetamer/')\n",
    "import utils.statnetencoder as sne\n",
    "import importlib\n",
    "import imp\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\chris\\Documents\\MIT\\Statistics_analysis_code\\PyTorchSteerablePyramid')\n",
    "sys.path.append('/home/gridsan/ckoevesdi/PyTorchSteerablePyramid/')\n",
    "import steerable\n",
    "import steerable.utils as utils\n",
    "from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "#use GPU 2\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb413b8-72ca-4a7a-b98f-d32cc73b5470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T14:47:05.643243100Z",
     "start_time": "2023-07-27T14:47:05.619221Z"
    }
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "num_epochs = 40\n",
    "batch_size = 20\n",
    "crop_size = 128 #changed to 64 as fourcrop doesn't exist\n",
    "num_stats = 50 \n",
    "optimizer_type='adam'\n",
    "#optimizer_type='sgd'\n",
    "learning_rate = 0.001\n",
    "num_crops = 5 #changed this to four\n",
    "\n",
    "sparsity_penalty = 0.001\n",
    "\n",
    "#dataset location\n",
    "#dtd_folder = '/gridsan/ckoevesdi/data/dtd_torch/dt/'\n",
    "dtd_folder = 'home/gridsan/ckoevesdi/data/OT/dtd_torch/dtd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806091f-63d7-4819-ad4e-2ef117a7097d",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1482b4a6-f29a-4448-be62-51cf8ae7defd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T15:37:37.244268300Z",
     "start_time": "2023-07-27T15:36:26.383047900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882\n"
     ]
    }
   ],
   "source": [
    "#loading_transforms = torchvision.transforms.Compose([#transforms.CenterCrop(size=300),\n",
    "#                                                    #transforms.RandomRotation(degrees=180),\n",
    "#                                                    transforms.Grayscale(), \n",
    "#                                                    #transforms.TenCrop(size=crop_size),\n",
    "#                                                    #transforms.RandomRotation(degrees=[0,90,180,270]),\n",
    "                                                    #transforms.RandomVerticalFlip(p=0.5), #Commented out\n",
    "                                                    #transforms.RandomHorizontalFlip(p=0.5), #Commented out\n",
    "#                                                    transforms.FiveCrop(size=crop_size), #changed back to fivecrop\n",
    "                                                    #transforms.functional.vflip(),\n",
    "                                                    #transforms.functional.hflip(),\n",
    " #                                                   transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),\n",
    "#                                                    transforms.ConvertImageDtype(torch.float32)])\n",
    "                                                    #transforms.PILToTensor()])\n",
    "loading_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Slight brightness & contrast adjustment\n",
    "    transforms.FiveCrop(size=crop_size), \n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])), # Changed PILToTensor to ToTensor as it's more standard\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='train', partition=10, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "\n",
    "sampler = data.RandomSampler(dtd_dataset)\n",
    "\n",
    "dtd_dataloader = DataLoader(dtd_dataset, \n",
    "                            sampler=sampler,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(dtd_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82c7a11-df35-4a7f-a71d-0dbeb8b5ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879\n"
     ]
    }
   ],
   "source": [
    "loading_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Slight brightness & contrast adjustment\n",
    "    transforms.FiveCrop(size=crop_size), \n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])), # Changed PILToTensor to ToTensor as it's more standard\n",
    "    transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "#use training set for now\n",
    "dtd_dataset = torchvision.datasets.DTD(root='/home/gridsan/ckoevesdi/data/OT/dtd_torch/', split='train', partition=1, \n",
    "                                       transform=loading_transforms, target_transform=None,\n",
    "                                       download=False) #ah das datenset muss so aussehen wie es auf der website auch ist, deswegen kann man auch download false machen\n",
    "\n",
    "\n",
    "dtd_dataloader = DataLoader(dtd_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "\n",
    "tensor2pil_transform = transforms.ToPILImage()\n",
    "print(len(dtd_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de6860-43e3-49ca-bbf5-00c492f2d669",
   "metadata": {},
   "source": [
    "## Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad27ed-cc59-4d7f-95c7-5ed5d29c3962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T15:46:30.645242Z",
     "start_time": "2023-07-27T15:46:18.747637400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(True):\n",
    "    for n, texture_batch in enumerate(dtd_dataloader):\n",
    "         #grab texture batch and generate matching labels\n",
    "         output = texture_batch[0].to(device)\n",
    "         output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "         texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "         #apply random permutation\n",
    "         print(texture_labels)\n",
    "         #loop through batch and plot images\n",
    "         for j in range(batch_size):\n",
    "             plt.figure(figsize=(8,4))\n",
    "             for i in range(num_crops):\n",
    "                 plt.subplot(2,5,i+1)\n",
    "                 plt.imshow(tensor2pil_transform(output[i+j*num_crops,:,:,:]))\n",
    "                 plt.axis('off')\n",
    "             plt.show()\n",
    "         if(n==1):\n",
    "             break;\n",
    "    \n",
    "#tensor2pil_transform(output[4,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d8ef7-aa2d-4db4-aa3d-b9c0c818eb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "dataloader_iterator2 = iter(dtd_dataloader)\n",
    "\n",
    "# Get the first batch\n",
    "first_batch = next(dataloader_iterator2)\n",
    "\n",
    "# Extract data and label from the first batch\n",
    "data, label = first_batch\n",
    "first_sample, first_label = data[0], label[0]\n",
    "# Assuming x is your (1, 128, 128) shaped matrix\n",
    "#plt.imshow(first_sample[0,0, :, :], cmap='gray'); plt.show()\n",
    "\n",
    "first_sample = first_sample.to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "print(first_sample[:,:, :, :].shape)\n",
    "print(first_label)\n",
    "x = statnet_model.encoder(first_sample[1,:, :, :])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f1091-0831-4a35-b4c2-a9c593fd16e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f2a6c-df93-4147-a57a-cd4cb1e7b976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T15:53:23.601154200Z",
     "start_time": "2023-07-27T15:53:23.081253Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(sne)\n",
    "statnet_model = sne.StatNetEncoder(img_size=(crop_size,crop_size),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_stats=num_stats,\n",
    "                                   device=device)\n",
    "statnet_model.to(device)\n",
    "stat_labels = statnet_model.getsstatlabels(device)\n",
    "#optimizer\n",
    "if(optimizer_type=='sgd'):\n",
    "    optimizer = torch.optim.SGD(statnet_model.parameters(), lr=learning_rate)#, momentum=learning_momentum)\n",
    "elif(optimizer_type=='adam'):\n",
    "    optimizer = torch.optim.Adam(statnet_model.parameters(), lr=learning_rate)\n",
    "elif(optimizer_type=='adagrad'):\n",
    "    optimizer = torch.optim.Adagrad(statnet_model.parameters(), lr=learning_rate)\n",
    "elif(optimizer_type=='adadelta'):\n",
    "    optimizer = torch.optim.Adadelta(statnet_model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    print('No Optimizer Specified! Adam is default!')\n",
    "    optimizer = torch.optim.Adam(statnet_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4780a5-6b0e-4ad8-9740-e59aaac2ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Stats:',len(stat_labels))\n",
    "summation=0\n",
    "for stat in ['edge_mean','edge_variance','edge_stop','edge_correlation','phase_correlation','mean','variance','bandpass_variance','skew','kurtosis']:\n",
    "    num_this_stat = len([s for s in stat_labels if s.weight_category==stat])\n",
    "    print(f'{stat} Stat:',num_this_stat)\n",
    "    summation += num_this_stat\n",
    "print('Stats Sum to:',summation)\n",
    "#[s for s in stat_labels] #print all stats with details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cac1f-098b-4e67-be13-818c250fb3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c69b4f-f89b-4a34-bde3-1916217c1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_contrastive = losses.GeneralizedLiftedStructureLoss()\n",
    "training_loss = []\n",
    "training_representation_loss = []\n",
    "training_sparsity_loss = []\n",
    "num_total_epochs = 0\n",
    "#[s for s in stat_labels] #print all stats with details\n",
    "statnet_model.train() # Set model to training mode\n",
    "optimizer.zero_grad()\n",
    "statnet_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8483c7-bda3-4b18-bc7b-b1a7f58c6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print('Initial Weights:', statnet_model._w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551e756-4a61-4650-b561-701e6d7d8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training:')\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    for j, texture_batch in enumerate(dtd_dataloader):\n",
    "        #grab texture batch and generate matching labels\n",
    "        output = texture_batch[0].to(device)\n",
    "        #print(output.shape)\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        texture_labels = torch.repeat_interleave(torch.arange(batch_size),num_crops)\n",
    "        perm = torch.randperm(batch_size * num_crops)\n",
    "\n",
    "        current_batch_size = output.shape[0] // num_crops\n",
    "        \n",
    "        if current_batch_size < batch_size:\n",
    "            continue  # Skip this batch if it's smaller than the usual batch size\n",
    "\n",
    "        output = output[perm]\n",
    "        texture_labels = texture_labels[perm]\n",
    "        #print(texture_labels.shape)\n",
    "        #calculate stats\n",
    "        stats_vector = statnet_model.forward(output) #statsvector torch.Size([batchsize*crops, 50])\n",
    "        #print(texture_labels.shape)\n",
    "        print(stats_vector.shape)\n",
    "        \n",
    "        #loss definitions\n",
    "        representation_loss = loss_func_contrastive(stats_vector, texture_labels)\n",
    "        sparsity_loss = statnet_model.sparse_loss()\n",
    "        loss = representation_loss + (sparsity_penalty * sparsity_loss) \n",
    "        #loss = triplet_loss(stats_vector, texture_labels, texture_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "\n",
    "        print('*',end='')\n",
    "        #training_loss.append(loss.item())      \n",
    "        if(j%30==0):\n",
    "            print(loss.item())\n",
    "        if(j==100):\n",
    "            break;\n",
    "        training_representation_loss.append(representation_loss.item())\n",
    "        training_sparsity_loss.append(loss.item())    \n",
    "        training_loss.append(loss.item())\n",
    "    num_total_epochs = num_total_epochs + 1\n",
    "    print(f'Finished Epoch {i}. Loss at {loss}.')\n",
    "    #print('Initial Weights:', statnet_model._w)    \n",
    "    if(i%10==0):\n",
    "        compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "        plt.imshow(compressor_mat)\n",
    "        plt.colorbar() \n",
    "        plt.show()\n",
    "print('All Done!')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047022-f77a-4eb7-955c-3889f33335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef973f0-65ab-44a2-b81f-eca0581488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor_mat = statnet_model.w.T.data.cpu().numpy()\n",
    "print(compressor_mat.shape)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(compressor_mat)\n",
    "plt.colorbar()\n",
    "print(compressor_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805de811-fdbf-48ab-baeb-517e15242ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_sums = np.sum(np.abs(compressor_mat),axis=0) #how weighted is each of the 150 stats?\n",
    "stat_index_array = np.argsort(stat_sums)[::-1] #get their importance order\n",
    "\n",
    "ordered_stat_sums = stat_sums[stat_index_array]\n",
    "ordered_stat_labels = np.array(stat_labels,dtype=object)[stat_index_array]\n",
    "ordered_stats_labels_pretty = [' '.join([''.join(str(item)) for item in row if item not in ['',None]]) for row in ordered_stat_labels]\n",
    "n=25\n",
    "print(f'Most Important {n} Stats:')\n",
    "for i in range(n):\n",
    "    print(ordered_stat_sums[i],ordered_stats_labels_pretty[i])\n",
    "\n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[:n]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "top_stats = ordered_stats_labels_pretty[:n]\n",
    "ax.set_yticks(np.arange(len(top_stats)), labels=top_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    \n",
    "    \n",
    "#plot most important stats for contrastive learning\n",
    "#plt.imshow(compressor_mat[:,stat_index_array[:n]].T)\n",
    "\n",
    "print(f'Least Important {n} Stats:')\n",
    "for i in range(1,n+1):\n",
    "    print(ordered_stat_sums[-i],ordered_stats_labels_pretty[-i])\n",
    "    \n",
    "    \n",
    "#plot the most important stats for contrastive learning\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(compressor_mat[:,stat_index_array[-n:]].T, aspect='auto')\n",
    "fig.colorbar(im)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "bot_stats = ordered_stats_labels_pretty[-n:][::-1]\n",
    "ax.set_yticks(np.arange(len(bot_stats)), labels=bot_stats)\n",
    "#Rotate the tick labels and set their alignment.\n",
    "#plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "#         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317dc4c-4cbd-4429-b13d-deb0e2ea987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?list.insert\n",
    "print(output.shape)\n",
    "\n",
    "print(texture_label_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626e6b-8949-426a-a12d-b05c2e84c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cpu = X.detach().cpu().numpy()\n",
    "print(np.any(np.isnan(X_cpu)))\n",
    "print(np.any(np.isinf(X_cpu)))\n",
    "print(X_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08629fca-112c-4060-adfb-2f7aae40b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# 1. Get model embeddings\n",
    "stats_vector = statnet_model.forward(output)\n",
    "X = stats_vector  # Assuming shape (n_samples, d)\n",
    "\n",
    "# 2. Compute t-SNE embeddings\n",
    "X_emb = TSNE(n_components=2, perplexity=8, n_iter=10000, verbose=True).fit_transform(X)\n",
    "\n",
    "# 3. Adjust texture labels\n",
    "texture_label = texture_labels[:-5]\n",
    "texture_label_list = texture_label.numpy()\n",
    "\n",
    "# 4. Get unique textures\n",
    "unique_textures = set(texture_label_list)\n",
    "\n",
    "# 5. Plot t-SNE embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for texture in unique_textures:\n",
    "    texture_index = np.where(texture_label_list == texture)[0]\n",
    "    plt.scatter(X_emb[texture_index, 0], X_emb[texture_index, 1], label=str(texture))\n",
    "\n",
    "# Optional: If you have too many labels, the legend might become cluttered.\n",
    "# If you have fewer labels, uncomment the next line.\n",
    "# plt.legend(loc='upper right')\n",
    "\n",
    "plt.title(f'T-sne embeddings: {num_crops} crops, {batch_size} textures')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2dc81-4e43-4fbb-b321-1a73883bfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_vector.shape)\n",
    "print(X_emb)\n",
    "#print(unique_textures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e10161-e890-4977-af82-c0cae58a4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_textures = set(texture_label_list)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for texture in unique_textures:\n",
    "    texture_index = np.where(texture_label_list == texture)[0]\n",
    "    plt.scatter(X_emb[texture_index, 0], X_emb[texture_index, 1], label=str(texture))\n",
    "    \n",
    "    # Optional: Annotate the centroid of each texture cluster instead of all points\n",
    "    centroid = X_emb[texture_index].mean(axis=0)\n",
    "    plt.annotate(str(texture), (centroid[0], centroid[1]))\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'T-sne embeddings: {num_crops} crops, {batch_size} textures')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299c9b4-0eb6-4b8d-a754-c9467e0944ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
